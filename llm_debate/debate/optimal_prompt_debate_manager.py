"""
ìµœì  í”„ë¡¬í”„íŠ¸ í† ë¡  ê´€ë¦¬ì

ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìƒˆë¡œìš´ í† ë¡  ì‹œìŠ¤í…œ:
1. ê° ì—ì´ì „íŠ¸ê°€ ìµœì´ˆ í”„ë¡¬í”„íŠ¸ ìƒì„± â†’ ê°€ì„¤ ìƒì„±
2. 3ë²ˆì˜ ì²´ê³„ì  í† ë¡  (ì§ì ‘ ì¸ìš© ê¸°ë°˜ í‰ê°€)
3. í† ë¡  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸ ìƒì„±
"""

from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
import time
import json
from ..agents.base_agent import BaseAgent
from ..agents.structural_agent import StructuralAgent
from ..agents.biological_agent import BiologicalAgent

@dataclass
class InitialPromptWithHypothesis:
    """ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ì™€ ìƒì„±ëœ ê°€ì„¤"""
    agent_name: str
    expertise: str
    initial_prompt: str
    generated_hypothesis: str
    timestamp: float

@dataclass
class DebateRound:
    """í† ë¡  ë¼ìš´ë“œ ì •ë³´"""
    round_number: int
    focus_agent: str  # ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ í‰ê°€ë°›ëŠ” ì—ì´ì „íŠ¸
    focus_prompt: str
    focus_hypothesis: str
    evaluations: List[Dict]  # ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì˜ í‰ê°€
    timestamp: float

@dataclass
class DirectQuoteEvaluation:
    """ì§ì ‘ ì¸ìš© ê¸°ë°˜ í‰ê°€"""
    evaluator_agent: str
    target_agent: str
    quoted_text: str  # ì§ì ‘ ì¸ìš©ëœ í…ìŠ¤íŠ¸
    evaluation_type: str  # "praise" ë˜ëŠ” "criticism"
    reasoning: str  # í‰ê°€ ê·¼ê±°
    improvement_suggestion: str  # ê°œì„  ì œì•ˆ (ë¹„íŒì¸ ê²½ìš°)

@dataclass
class OptimalPromptDebateState:
    """ìµœì  í”„ë¡¬í”„íŠ¸ í† ë¡  ìƒíƒœ"""
    activity_cliff: Dict = None
    context_info: Dict = None
    target_name: str = ""
    
    # 1ë‹¨ê³„: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ë° ê°€ì„¤ ìƒì„±
    initial_prompts_with_hypotheses: List[InitialPromptWithHypothesis] = field(default_factory=list)
    
    # 2ë‹¨ê³„: 3ë²ˆì˜ í† ë¡  ë¼ìš´ë“œ
    debate_rounds: List[DebateRound] = field(default_factory=list)
    
    # 3ë‹¨ê³„: ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸
    final_optimal_prompt: str = ""
    final_optimal_hypothesis: str = ""
    
    # ë©”íƒ€ë°ì´í„°
    debate_id: str = ""
    start_time: float = 0.0
    end_time: float = 0.0
    errors: List[str] = field(default_factory=list)

class OptimalPromptDebateManager:
    """
    ìµœì  í”„ë¡¬í”„íŠ¸ í† ë¡  ê´€ë¦¬ì
    
    ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì„ ì •í™•íˆ êµ¬í˜„:
    1. ê° ì—ì´ì „íŠ¸ ìµœì´ˆ í”„ë¡¬í”„íŠ¸ + ê°€ì„¤ ìƒì„± ë° í™”ë©´ í‘œì‹œ
    2. 3ê°œ ì—ì´ì „íŠ¸ Ã— 3ë²ˆ í† ë¡  (ê°ìë¥¼ ì£¼ì œë¡œ)
    3. ì§ì ‘ ì¸ìš© ê¸°ë°˜ íˆ¬ëª…í•œ í‰ê°€
    4. í† ë¡  ê²°ê³¼ ì¢…í•©í•˜ì—¬ ìµœì  í”„ë¡¬í”„íŠ¸ 1ê°œ ìƒì„±
    5. ìµœì¢… í”„ë¡¬í”„íŠ¸ + ê°€ì„¤ ì „ë¬¸ ì œì‹œ
    """
    
    def __init__(self):
        self.agents: Dict[str, BaseAgent] = {}
        self.debate_topic = "ìë™í™”ëœ ì§€ëŠ¥í˜• SAR ë¶„ì„ ì‹œìŠ¤í…œì„ ìœ„í•œ ìµœì  ê·¼ê±° ì¤‘ì‹¬ ê°€ì„¤ ìƒì„± ë°©ë²•ë¡  í™•ë¦½"
    
    def setup_agents_unified(self, llm_provider: str, api_key: str):
        """ì„ íƒëœ ë‹¨ì¼ LLMìœ¼ë¡œ 3ê°œ ì—ì´ì „íŠ¸ í†µì¼ ì„¤ì •"""
        if llm_provider == "OpenAI":
            model_provider = "openai"
            model_name = "gpt-4o"
        elif llm_provider in ["Gemini", "Google Gemini"]:
            model_provider = "gemini"
            model_name = "gemini-2.5-pro"  # ìµœì‹  ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ì: {llm_provider}")
        
        # ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ ì„ íƒëœ ë‹¨ì¼ ëª¨ë¸ë¡œ í†µì¼
        self.agents = {
            "structural": StructuralAgent(
                model_provider=model_provider,
                model_name=model_name,
                api_key=api_key,
                temperature=0.3
            ),
            "biological": BiologicalAgent(
                model_provider=model_provider,
                model_name=model_name,
                api_key=api_key,
                temperature=0.3
            ),
            "sar": BiologicalAgent(  # FutureHouseAgent ëŒ€ì‹  BiologicalAgent ì‚¬ìš©
                model_provider=model_provider,
                model_name=model_name,
                api_key=api_key,
                temperature=0.3
            )
        }
        
        # SAR ì—ì´ì „íŠ¸ì˜ ì „ë¬¸ì„±ì„ SARë¡œ ë³€ê²½
        self.agents["sar"].expertise = "êµ¬ì¡°-í™œì„± ê´€ê³„ (SAR) í†µí•©"
    
    def run_optimal_prompt_debate(self, 
                                 activity_cliff: Dict, 
                                 context_info: Dict = None,
                                 target_name: str = "") -> OptimalPromptDebateState:
        """ì „ì²´ ìµœì  í”„ë¡¬í”„íŠ¸ í† ë¡  ì‹¤í–‰"""
        state = OptimalPromptDebateState(
            activity_cliff=activity_cliff,
            context_info=context_info,
            target_name=target_name,
            debate_id=f"optimal_debate_{int(time.time())}",
            start_time=time.time()
        )
        
        try:
            # 1ë‹¨ê³„: ê° ì—ì´ì „íŠ¸ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ + ê°€ì„¤ ìƒì„±
            state = self._generate_initial_prompts_and_hypotheses(state)
            
            # 2ë‹¨ê³„: 3ë²ˆì˜ í† ë¡  ë¼ìš´ë“œ ì‹¤í–‰
            state = self._execute_three_debate_rounds(state)
            
            # 3ë‹¨ê³„: ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸ ìƒì„±
            state = self._generate_final_optimal_prompt(state)
            
        except Exception as e:
            state.errors.append(f"í† ë¡  ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        
        state.end_time = time.time()
        return state
    
    def _generate_initial_prompts_and_hypotheses(self, state: OptimalPromptDebateState) -> OptimalPromptDebateState:
        """1ë‹¨ê³„: ê° ì—ì´ì „íŠ¸ê°€ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ìƒì„± â†’ ê°€ì„¤ ìƒì„±"""
        print("ğŸ¯ 1ë‹¨ê³„: ê° ì—ì´ì „íŠ¸ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ë° ê°€ì„¤ ìƒì„±")
        
        for agent_name, agent in self.agents.items():
            try:
                print(f"  ğŸ“ {agent.expertise} ì „ë¬¸ê°€ - ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
                
                # 1-1. ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ìƒì„±
                initial_prompt = self._generate_initial_prompt_for_agent(agent, state.activity_cliff, state.target_name)
                
                # 1-2. ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¡œ ê°€ì„¤ ìƒì„±
                hypothesis = self._generate_hypothesis_with_prompt(initial_prompt, state.activity_cliff, state.context_info, state.target_name)
                
                # 1-3. ê²°ê³¼ ì €ì¥
                initial_data = InitialPromptWithHypothesis(
                    agent_name=agent_name,
                    expertise=agent.expertise,
                    initial_prompt=initial_prompt,
                    generated_hypothesis=hypothesis,
                    timestamp=time.time()
                )
                state.initial_prompts_with_hypotheses.append(initial_data)
                
                print(f"  âœ… {agent.expertise} í”„ë¡¬í”„íŠ¸ ë° ê°€ì„¤ ìƒì„± ì™„ë£Œ")
                
            except Exception as e:
                state.errors.append(f"{agent_name} ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                print(f"  âŒ {agent.expertise} ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        print(f"ğŸ¯ 1ë‹¨ê³„ ì™„ë£Œ: {len(state.initial_prompts_with_hypotheses)}ê°œ í”„ë¡¬í”„íŠ¸+ê°€ì„¤ ìƒì„±")
        return state
    
    def _generate_initial_prompt_for_agent(self, agent: BaseAgent, activity_cliff: Dict, target_name: str = "") -> str:
        """ì—ì´ì „íŠ¸ë³„ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        target_protein_info = f"**íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ**: {target_name}" if target_name else "**íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ**: ì •ë³´ ì—†ìŒ"
        
        prompt_generation_instruction = f"""
ë‹¹ì‹ ì€ {agent.expertise} ì „ë¬¸ê°€ë¡œì„œ, Activity Cliff ë¶„ì„ì„ ìœ„í•œ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤.

**í† ë¡  ì£¼ì œ**: {self.debate_topic}

**Activity Cliff ë°ì´í„°**:
- í™”í•©ë¬¼ 1: {activity_cliff['mol_1']['ID']} (pKi: {activity_cliff['mol_1']['pKi']})
- í™”í•©ë¬¼ 2: {activity_cliff['mol_2']['ID']} (pKi: {activity_cliff['mol_2']['pKi']})
- êµ¬ì¡° ìœ ì‚¬ë„: {activity_cliff['similarity']:.3f}
- í™œì„±ë„ ì°¨ì´: {activity_cliff['activity_diff']:.2f}
{target_protein_info}

**ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼**: {agent.expertise}

**í•µì‹¬ ëª©í‘œ**: ì‹ ì•½ ê°œë°œ ì „ë¬¸ê°€ê°€ ìˆ˜í–‰í•˜ë˜ ìˆ˜ê¸° SAR ë¶„ì„ì„ ìë™í™”í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. ì‹¤ì œ ì‹ ì•½ ê°œë°œì—ì„œ í™œìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì  ê°€ì„¤ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

**ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•  í•µì‹¬ ìš”ì†Œ**:
1. **í™œì„± ë³€í™” ì£¼ìš” ìš”ì¸ (Key Activity Cliff)**: í™œì„±ë„ ì°¨ì´ì˜ í•µì‹¬ ì›ì¸ì„ ëª…í™•íˆ ì‹ë³„
2. **ì£¼ìš” êµ¬ì¡° ë³€ê²½ íŠ¸ë Œë“œ (General SAR Trend)**: êµ¬ì¡° ë³€ê²½ì´ í™œì„±ì— ë¯¸ì¹˜ëŠ” ì¼ë°˜ì  íŒ¨í„´
3. **íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ ì •ë³´ í™œìš©**: ì œê³µëœ íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ íŠ¹ì„±ì„ ê°€ì„¤ ìƒì„± ê·¼ê±°ë¡œ ë°˜ë“œì‹œ í™œìš©

**ê°€ì„¤ í˜•ì‹ ìš”êµ¬ì‚¬í•­**:
ê°€ì„¤ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì˜ ë„ì…ë¶€ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤:
```
- **í™”í•©ë¬¼ 1 ({activity_cliff['mol_1']['ID']}):** pKi = {activity_cliff['mol_1']['pKi']}
- **í™”í•©ë¬¼ 2 ({activity_cliff['mol_2']['ID']}):** pKi = {activity_cliff['mol_2']['pKi']}
- **êµ¬ì¡° ìœ ì‚¬ë„:** {activity_cliff['similarity']:.3f} (ì‚¬ì‹¤ìƒ ë™ì¼ êµ¬ì¡°)
- **í™œì„±ë„ ì°¨ì´:** {activity_cliff['activity_diff']:.2f} pKi ë‹¨ìœ„ (ì•½ {10**(activity_cliff['activity_diff']):.0f}ë°° ì°¨ì´)
{target_protein_info}
```

**ìš”êµ¬ì‚¬í•­**:
1. ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ ê´€ì ì—ì„œ Activity Cliff í˜„ìƒì„ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•˜ì„¸ìš”
2. í”„ë¡¬í”„íŠ¸ëŠ” ë°˜ë“œì‹œ 5ë‹¨ê³„ CoT(Chain of Thought) êµ¬ì¡°ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤
3. êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ê°€ì„¤ì„ ìƒì„±í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
4. ê³¼í•™ì  ê·¼ê±°ì™€ ì‹¤í—˜ì  ê²€ì¦ ë°©ë²•ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤
5. íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ ì •ë³´ë¥¼ ê°€ì„¤ ìƒì„± ê·¼ê±°ë¡œ í™œìš©í•˜ë„ë¡ ì§€ì‹œí•´ì•¼ í•©ë‹ˆë‹¤
6. "í™œì„± ë³€í™” ì£¼ìš” ìš”ì¸"ê³¼ "êµ¬ì¡° ë³€ê²½ íŠ¸ë Œë“œ"ë¥¼ í•µì‹¬ìœ¼ë¡œ ì œì‹œí•˜ë„ë¡ ì§€ì‹œí•´ì•¼ í•©ë‹ˆë‹¤

**í”„ë¡¬í”„íŠ¸ í˜•ì‹**:
```
ë‹¹ì‹ ì€ [ì „ë¬¸ ë¶„ì•¼] ì „ë¬¸ê°€ì…ë‹ˆë‹¤...

ë‹¤ìŒ 5ë‹¨ê³„ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1ë‹¨ê³„: [êµ¬ì²´ì  ì§€ì‹œ]
2ë‹¨ê³„: [êµ¬ì²´ì  ì§€ì‹œ]
3ë‹¨ê³„: [êµ¬ì²´ì  ì§€ì‹œ]
4ë‹¨ê³„: [êµ¬ì²´ì  ì§€ì‹œ]
5ë‹¨ê³„: [êµ¬ì²´ì  ì§€ì‹œ]

[ì¶”ê°€ ì§€ì¹¨ë“¤...]
```

**{agent.expertise} ì „ë¬¸ê°€ë¡œì„œ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•˜ì„¸ìš”:**
"""
        
        try:
            response = agent._call_llm("ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.", prompt_generation_instruction)
            return response
        except Exception as e:
            return f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def _generate_hypothesis_with_prompt(self, prompt: str, activity_cliff: Dict, context_info: Dict, target_name: str = "") -> str:
        """ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¡œ ê°€ì„¤ ìƒì„±"""
        try:
            # Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì„¤ ìƒì„±
            try:
                import google.generativeai as genai
                # API í‚¤ ì„¤ì • (biological ì—ì´ì „íŠ¸ì˜ API í‚¤ ì‚¬ìš©)
                gemini_agent = self.agents.get("biological")
                if gemini_agent and gemini_agent.api_key:
                    genai.configure(api_key=gemini_agent.api_key)
                model = genai.GenerativeModel("gemini-2.0-flash")
            except ImportError:
                return "Google Gemini API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-generativeai ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            
            activity_cliff_description = f"""
Activity Cliff ì •ë³´:
- í™”í•©ë¬¼ 1: {activity_cliff['mol_1']['ID']} (SMILES: {activity_cliff['mol_1']['SMILES']}, pKi: {activity_cliff['mol_1']['pKi']})  
- í™”í•©ë¬¼ 2: {activity_cliff['mol_2']['ID']} (SMILES: {activity_cliff['mol_2']['SMILES']}, pKi: {activity_cliff['mol_2']['pKi']})
- êµ¬ì¡° ìœ ì‚¬ë„: {activity_cliff['similarity']:.3f}
- í™œì„±ë„ ì°¨ì´: {activity_cliff['activity_diff']:.2f}
"""
            
            if target_name:
                activity_cliff_description += f"- íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ: {target_name}\n"
            
            context_text = ""
            if context_info and context_info.get('related_papers'):
                context_text = "\nê´€ë ¨ ë¬¸í—Œ ì •ë³´:\n" + "\n".join([
                    f"- {paper.get('title', '')}: {paper.get('abstract', '')[:200]}..." 
                    for paper in context_info['related_papers'][:3]
                ])
            
            # ê°€ì„¤ ìƒì„±ë§Œì„ ìœ„í•œ ëª…í™•í•œ ì§€ì‹œì‚¬í•­
            hypothesis_generation_instruction = f"""
**ì¤‘ìš”**: ìœ„ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê°€ì„¤ì„ ìƒì„±í•˜ì„¸ìš”. í”„ë¡¬í”„íŠ¸ ìì²´ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ êµ¬ì²´ì ì¸ ê°€ì„¤ì„ ì‘ì„±í•˜ì„¸ìš”.

**ê°€ì„¤ ìƒì„± í˜•ì‹**:
- **í™”í•©ë¬¼ 1 ({activity_cliff['mol_1']['ID']}):** pKi = {activity_cliff['mol_1']['pKi']}
- **í™”í•©ë¬¼ 2 ({activity_cliff['mol_2']['ID']}):** pKi = {activity_cliff['mol_2']['pKi']}
- **êµ¬ì¡° ìœ ì‚¬ë„:** {activity_cliff['similarity']:.3f} (ì‚¬ì‹¤ìƒ ë™ì¼ êµ¬ì¡°)
- **í™œì„±ë„ ì°¨ì´:** {activity_cliff['activity_diff']:.2f} pKi ë‹¨ìœ„ (ì•½ {10**(activity_cliff['activity_diff']):.0f}ë°° ì°¨ì´)
{f"- **íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ**: {target_name}" if target_name else ""}

**ë°˜ë“œì‹œ ë‹¤ìŒ ë‘ í•µì‹¬ ìš”ì†Œë¥¼ í¬í•¨í•œ êµ¬ì²´ì ì¸ ê°€ì„¤ì„ ì‘ì„±í•˜ì„¸ìš”**:
1. **í™œì„± ë³€í™” ì£¼ìš” ìš”ì¸ (Key Activity Cliff)**: [êµ¬ì²´ì ì¸ ë¶„ì„]
2. **ì£¼ìš” êµ¬ì¡° ë³€ê²½ íŠ¸ë Œë“œ (General SAR Trend)**: [êµ¬ì²´ì ì¸ ë¶„ì„]

**ì¤‘ìš”**: ê°€ì„¤ë§Œ ì‘ì„±í•˜ê³ , í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
"""
            
            full_prompt = f"{prompt}\n\n{activity_cliff_description}\n{context_text}\n\n{hypothesis_generation_instruction}"
            
            response = model.generate_content(full_prompt)
            return response.text
            
        except Exception as e:
            return f"ê°€ì„¤ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def _execute_three_debate_rounds(self, state: OptimalPromptDebateState) -> OptimalPromptDebateState:
        """2ë‹¨ê³„: 3ë²ˆì˜ í† ë¡  ë¼ìš´ë“œ ì‹¤í–‰"""
        print("ğŸ­ 2ë‹¨ê³„: 3ë²ˆì˜ ì²´ê³„ì  í† ë¡  ì‹¤í–‰")
        
        # ê° ì—ì´ì „íŠ¸ë¥¼ ì£¼ì œë¡œ í•˜ëŠ” 3ë²ˆì˜ í† ë¡ 
        for round_num, focus_agent_name in enumerate(self.agents.keys(), 1):
            print(f"\n  ğŸ”„ í† ë¡  {round_num}ë¼ìš´ë“œ: {self.agents[focus_agent_name].expertise} ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ ì§‘ì¤‘ í‰ê°€")
            
            # í•´ë‹¹ ë¼ìš´ë“œì˜ ì£¼ì œê°€ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ì™€ ê°€ì„¤ ì°¾ê¸°
            focus_data = next(
                item for item in state.initial_prompts_with_hypotheses 
                if item.agent_name == focus_agent_name
            )
            
            # ë‹¤ë¥¸ 2ëª…ì˜ ì—ì´ì „íŠ¸ê°€ í‰ê°€
            evaluations = []
            for evaluator_name, evaluator_agent in self.agents.items():
                if evaluator_name != focus_agent_name:
                    print(f"    ğŸ“Š {evaluator_agent.expertise} â†’ {focus_data.expertise} í‰ê°€ ì¤‘...")
                    
                    evaluation = self._conduct_direct_quote_evaluation(
                        evaluator_agent, focus_data, state
                    )
                    evaluations.append(evaluation)
            
            # í† ë¡  ë¼ìš´ë“œ ê²°ê³¼ ì €ì¥
            debate_round = DebateRound(
                round_number=round_num,
                focus_agent=focus_agent_name,
                focus_prompt=focus_data.initial_prompt,
                focus_hypothesis=focus_data.generated_hypothesis,
                evaluations=evaluations,
                timestamp=time.time()
            )
            state.debate_rounds.append(debate_round)
            
            print(f"  âœ… í† ë¡  {round_num}ë¼ìš´ë“œ ì™„ë£Œ")
        
        print(f"ğŸ­ 2ë‹¨ê³„ ì™„ë£Œ: ì´ {len(state.debate_rounds)}ë²ˆì˜ í† ë¡  ì™„ë£Œ")
        return state
    
    def _conduct_direct_quote_evaluation(self, evaluator_agent: BaseAgent, 
                                        focus_data: InitialPromptWithHypothesis, 
                                        state: OptimalPromptDebateState) -> Dict:
        """ì§ì ‘ ì¸ìš© ê¸°ë°˜ íˆ¬ëª…í•œ í‰ê°€ ì‹¤ì‹œ"""
        
        evaluation_prompt = f"""
**í† ë¡  ì£¼ì œ**: {self.debate_topic}

ë‹¹ì‹ ì€ {evaluator_agent.expertise} ì „ë¬¸ê°€ë¡œì„œ, {focus_data.expertise} ì „ë¬¸ê°€ê°€ ì œì•ˆí•œ í”„ë¡¬í”„íŠ¸ì™€ ê°€ì„¤ì„ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

**í‰ê°€ ëŒ€ìƒ í”„ë¡¬í”„íŠ¸**:
```
{focus_data.initial_prompt}
```

**í•´ë‹¹ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„±ëœ ê°€ì„¤**:
```  
{focus_data.generated_hypothesis}
```

**í‰ê°€ ì§€ì¹¨**:
1. **ë°˜ë“œì‹œ ì§ì ‘ ì¸ìš©**í•˜ì—¬ í‰ê°€í•˜ì„¸ìš”
2. ì¹­ì°¬í•  ë¶€ë¶„ê³¼ ë¹„íŒí•  ë¶€ë¶„ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
3. ê° í‰ê°€ë§ˆë‹¤ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
4. {evaluator_agent.expertise} ì „ë¬¸ê°€ ê´€ì ì—ì„œ í‰ê°€í•˜ì„¸ìš”

**ì‘ë‹µ í˜•ì‹**:
```json
{{
    "evaluator_expertise": "{evaluator_agent.expertise}",
    "target_expertise": "{focus_data.expertise}",
    "praise_evaluations": [
        {{
            "quoted_text": "ì§ì ‘ ì¸ìš©ëœ í…ìŠ¤íŠ¸",
            "reasoning": "ì´ ë¶€ë¶„ì´ ìš°ìˆ˜í•œ êµ¬ì²´ì  ì´ìœ ",
            "score": 8.5
        }}
    ],
    "criticism_evaluations": [
        {{
            "quoted_text": "ì§ì ‘ ì¸ìš©ëœ í…ìŠ¤íŠ¸", 
            "reasoning": "ì´ ë¶€ë¶„ì˜ ë¬¸ì œì ê³¼ êµ¬ì²´ì  ì´ìœ ",
            "improvement_suggestion": "êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ",
            "score": 3.2
        }}
    ],
    "overall_assessment": "ì „ì²´ì ì¸ í‰ê°€ ë° {evaluator_agent.expertise} ê´€ì ì—ì„œì˜ ì˜ê²¬"
}}
```

**{evaluator_agent.expertise} ì „ë¬¸ê°€ë¡œì„œ íˆ¬ëª…í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”:**
"""
        
        try:
            # FutureHouse ì—ì´ì „íŠ¸ì¸ ê²½ìš° JSON í˜•ì‹ ì‘ë‹µì„ íŠ¹ë³„íˆ ìš”ì²­
            if evaluator_agent.model_provider == "futurehouse":
                system_prompt = f"ë‹¹ì‹ ì€ {evaluator_agent.expertise} ì „ë¬¸ê°€ì´ë©°, íˆ¬ëª…í•˜ê³  ê³µì •í•œ í‰ê°€ìì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
                json_instruction = "\n\n**ì¤‘ìš”**: ìœ„ì˜ JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”."
                response = evaluator_agent._call_llm(system_prompt, evaluation_prompt + json_instruction)
            else:
                response = evaluator_agent._call_llm(
                    f"ë‹¹ì‹ ì€ {evaluator_agent.expertise} ì „ë¬¸ê°€ì´ë©°, íˆ¬ëª…í•˜ê³  ê³µì •í•œ í‰ê°€ìì…ë‹ˆë‹¤.",
                    evaluation_prompt
                )
            
            # JSON ì‘ë‹µ íŒŒì‹±
            try:
                import re
                import json
                
                # ì—¬ëŸ¬ ê°€ì§€ JSON íŒ¨í„´ ì‹œë„
                json_patterns = [
                    r'```json\s*(\{.*?\})\s*```',  # ê¸°ë³¸ JSON ì½”ë“œ ë¸”ë¡
                    r'```\s*(\{.*?\})\s*```',      # json íƒœê·¸ ì—†ëŠ” ì½”ë“œ ë¸”ë¡
                    r'(\{[^{}]*"evaluator_expertise"[^{}]*\})',  # ê¸°ë³¸ JSON ê°ì²´ íŒ¨í„´
                ]
                
                evaluation_data = None
                for pattern in json_patterns:
                    json_match = re.search(pattern, response, re.DOTALL)
                    if json_match:
                        try:
                            evaluation_data = json.loads(json_match.group(1))
                            break
                        except json.JSONDecodeError:
                            continue
                
                # JSON íŒŒì‹±ì´ ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš° - íŠ¹íˆ FutureHouse ì—ì´ì „íŠ¸
                if not evaluation_data:
                    if evaluator_agent.model_provider == "futurehouse":
                        # FutureHouse ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
                        evaluation_data = self._parse_futurehouse_evaluation(response, evaluator_agent.expertise, focus_data.expertise)
                    else:
                        # ì¼ë°˜ì ì¸ íŒŒì‹± ì‹¤íŒ¨ ì²˜ë¦¬
                        evaluation_data = {
                            "evaluator_expertise": evaluator_agent.expertise,
                            "target_expertise": focus_data.expertise,
                            "raw_evaluation": response,
                            "parse_error": "JSON í˜•ì‹ íŒŒì‹± ì‹¤íŒ¨"
                        }
                        
            except Exception as e:
                evaluation_data = {
                    "evaluator_expertise": evaluator_agent.expertise,
                    "target_expertise": focus_data.expertise,
                    "raw_evaluation": response,
                    "parse_error": f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
                }
            
            return evaluation_data
            
        except Exception as e:
            return {
                "evaluator_expertise": evaluator_agent.expertise,
                "target_expertise": focus_data.expertise,
                "error": f"í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            }
    
    def _generate_final_optimal_prompt(self, state: OptimalPromptDebateState) -> OptimalPromptDebateState:
        """3ë‹¨ê³„: í† ë¡  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        print("ğŸ† 3ë‹¨ê³„: í† ë¡  ê²°ê³¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸ ìƒì„±")
        
        try:
            # ëª¨ë“  í† ë¡  ê²°ê³¼ ìˆ˜ì§‘
            all_praise_parts = []
            all_criticism_parts = []
            
            for debate_round in state.debate_rounds:
                for evaluation in debate_round.evaluations:
                    if 'praise_evaluations' in evaluation:
                        for praise in evaluation['praise_evaluations']:
                            all_praise_parts.append({
                                'source_expertise': debate_round.focus_agent,
                                'evaluator_expertise': evaluation.get('evaluator_expertise'),
                                'quoted_text': praise.get('quoted_text'),
                                'reasoning': praise.get('reasoning'),
                                'score': praise.get('score', 0)
                            })
                    
                    if 'criticism_evaluations' in evaluation:
                        for criticism in evaluation['criticism_evaluations']:
                            all_criticism_parts.append({
                                'source_expertise': debate_round.focus_agent,
                                'evaluator_expertise': evaluation.get('evaluator_expertise'),
                                'quoted_text': criticism.get('quoted_text'),
                                'reasoning': criticism.get('reasoning'),
                                'improvement_suggestion': criticism.get('improvement_suggestion'),
                                'score': criticism.get('score', 0)
                            })
            
            # ì‹¬íŒ ì—ì´ì „íŠ¸ê°€ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± 
            try:
                import google.generativeai as genai
                # API í‚¤ ì„¤ì • (biological ì—ì´ì „íŠ¸ì˜ API í‚¤ ì‚¬ìš©)
                gemini_agent = self.agents.get("biological")
                if gemini_agent and gemini_agent.api_key:
                    genai.configure(api_key=gemini_agent.api_key)
                model = genai.GenerativeModel("gemini-2.0-flash")
            except ImportError:
                state.errors.append("Google Gemini API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return state
            
            final_prompt_instruction = f"""
**í† ë¡  ì£¼ì œ**: {self.debate_topic}

ë‹¹ì‹ ì€ ì¤‘ë¦½ì ì¸ ì‹¬íŒìœ¼ë¡œì„œ, 3ëª…ì˜ ì „ë¬¸ê°€ê°€ ì§„í–‰í•œ í† ë¡  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ **ë‹¨ í•˜ë‚˜ì˜ ìµœì  í”„ë¡¬í”„íŠ¸**ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

**ì›ë³¸ í”„ë¡¬í”„íŠ¸ë“¤**:
"""
            
            for i, initial_data in enumerate(state.initial_prompts_with_hypotheses, 1):
                final_prompt_instruction += f"""
### {i}. {initial_data.expertise} ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸:
```
{initial_data.initial_prompt}
```
"""
            
            final_prompt_instruction += f"""

**í† ë¡ ì—ì„œ ì¢‹ì€ í‰ê°€ë¥¼ ë°›ì€ ë¶€ë¶„ë“¤**:
"""
            for i, praise in enumerate(all_praise_parts, 1):
                final_prompt_instruction += f"""
{i}. [{praise['source_expertise']}] "{praise['quoted_text']}"
   - í‰ê°€ì: {praise['evaluator_expertise']}
   - í‰ê°€ ì´ìœ : {praise['reasoning']}
   - ì ìˆ˜: {praise['score']}
   
"""
            
            final_prompt_instruction += f"""

**í† ë¡ ì—ì„œ ê°œì„ ì´ í•„ìš”í•˜ë‹¤ê³  ì§€ì ëœ ë¶€ë¶„ë“¤**:
"""
            for i, criticism in enumerate(all_criticism_parts, 1):
                final_prompt_instruction += f"""
{i}. [{criticism['source_expertise']}] "{criticism['quoted_text']}"
   - í‰ê°€ì: {criticism['evaluator_expertise']}
   - ë¬¸ì œì : {criticism['reasoning']}
   - ê°œì„  ë°©ì•ˆ: {criticism['improvement_suggestion']}
   
"""
            
            final_prompt_instruction += """

**ìµœì¢… ê³¼ì œ**:
í† ë¡  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ **ì„¸ ì „ë¬¸ê°€ì˜ ì¥ì ì„ ëª¨ë‘ í†µí•©í•œ ë‹¨ í•˜ë‚˜ì˜ ìµœì  í”„ë¡¬í”„íŠ¸**ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­**:
1. í† ë¡ ì—ì„œ ì¢‹ì€ í‰ê°€ë¥¼ ë°›ì€ ë¶€ë¶„ë“¤ì„ ìµœëŒ€í•œ í™œìš©
2. ì§€ì ëœ ë¬¸ì œì ë“¤ì€ ê°œì„  ë°©ì•ˆì„ ë°˜ì˜í•˜ì—¬ ìˆ˜ì •
3. 5ë‹¨ê³„ CoT êµ¬ì¡° í•„ìˆ˜ ìœ ì§€
4. êµ¬ì¡°í™”í•™, ìƒì²´ë¶„ì ìƒí˜¸ì‘ìš©, SAR í†µí•© ê´€ì ì„ ëª¨ë‘ í¬ê´„
5. ì‹¤ìš©ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ê°€ì„¤ ìƒì„± ê°€ëŠ¥

**ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:**
"""
            
            # Geminië¡œ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
            
            final_prompt_response = model.generate_content(final_prompt_instruction)
            state.final_optimal_prompt = final_prompt_response.text
            
            # ìµœì¢… í”„ë¡¬í”„íŠ¸ë¡œ ê°€ì„¤ ìƒì„±
            print("  ğŸ§ª ìµœì¢… í”„ë¡¬í”„íŠ¸ë¡œ ìµœì¢… ê°€ì„¤ ìƒì„± ì¤‘...")
            state.final_optimal_hypothesis = self._generate_hypothesis_with_prompt(
                state.final_optimal_prompt, 
                state.activity_cliff, 
                state.context_info,
                state.target_name
            )
            
            print("ğŸ† 3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… ìµœì  í”„ë¡¬í”„íŠ¸ ë° ê°€ì„¤ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            state.errors.append(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        return state
    
    def _parse_futurehouse_evaluation(self, response: str, evaluator_expertise: str, target_expertise: str) -> Dict:
        """FutureHouse ì—ì´ì „íŠ¸ì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í‰ê°€ í˜•íƒœë¡œ ë³€í™˜"""
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì¹­ì°¬ê³¼ ë¹„íŒ ë¶€ë¶„ì„ ì¶”ì¶œí•˜ë ¤ê³  ì‹œë„
        praise_evaluations = []
        criticism_evaluations = []
        
        lines = response.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ì„¹ì…˜ êµ¬ë¶„ í‚¤ì›Œë“œ ê°ì§€
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in ['ì¹­ì°¬', 'praise', 'ì¥ì ', 'ìš°ìˆ˜', 'positive']):
                if current_content and current_section:
                    self._add_evaluation_item(current_section, current_content, praise_evaluations, criticism_evaluations)
                current_section = 'praise'
                current_content = []
            elif any(keyword in line_lower for keyword in ['ë¹„íŒ', 'criticism', 'ë¬¸ì œ', 'ê°œì„ ', 'negative']):
                if current_content and current_section:
                    self._add_evaluation_item(current_section, current_content, praise_evaluations, criticism_evaluations)
                current_section = 'criticism'
                current_content = []
            else:
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if current_content and current_section:
            self._add_evaluation_item(current_section, current_content, praise_evaluations, criticism_evaluations)
        
        # ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ ì‘ë‹µì„ ì¼ë°˜ì ì¸ í‰ê°€ë¡œ ì²˜ë¦¬
        if not praise_evaluations and not criticism_evaluations:
            # ì‘ë‹µ í†¤ ë¶„ì„í•˜ì—¬ ì¹­ì°¬/ë¹„íŒ êµ¬ë¶„
            if any(keyword in response.lower() for keyword in ['ìš°ìˆ˜', 'ì¢‹', 'good', 'excellent', 'í›Œë¥­']):
                praise_evaluations.append({
                    "quoted_text": response[:200] + "..." if len(response) > 200 else response,
                    "reasoning": "ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì¸ í‰ê°€",
                    "score": 7.0
                })
            else:
                criticism_evaluations.append({
                    "quoted_text": response[:200] + "..." if len(response) > 200 else response,
                    "reasoning": "ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ë“¤ì— ëŒ€í•œ ì§€ì ",
                    "improvement_suggestion": "êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ í•„ìš”",
                    "score": 5.0
                })
        
        return {
            "evaluator_expertise": evaluator_expertise,
            "target_expertise": target_expertise,
            "praise_evaluations": praise_evaluations,
            "criticism_evaluations": criticism_evaluations,
            "overall_assessment": f"{evaluator_expertise} ê´€ì ì—ì„œì˜ ì¢…í•© í‰ê°€ (FutureHouse ì‘ë‹µ íŒŒì‹±)"
        }
    
    def _add_evaluation_item(self, section_type: str, content_lines: List[str], praise_list: List, criticism_list: List):
        """í‰ê°€ í•­ëª©ì„ í•´ë‹¹ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€"""
        content = ' '.join(content_lines).strip()
        if not content:
            return
            
        if section_type == 'praise':
            praise_list.append({
                "quoted_text": content[:150] + "..." if len(content) > 150 else content,
                "reasoning": "ê¸ì •ì  í‰ê°€ ìš”ì†Œ",
                "score": 7.5
            })
        elif section_type == 'criticism':
            criticism_list.append({
                "quoted_text": content[:150] + "..." if len(content) > 150 else content,
                "reasoning": "ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„",
                "improvement_suggestion": "êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ ê²€í†  í•„ìš”",
                "score": 4.0
            })