"""
Co-Scientist ê¸°ë°˜ SAR ë¶„ì„ ì‹œìŠ¤í…œ - 3ë‹¨ê³„ ì „ë¬¸ê°€ í˜‘ì—… ê°€ì„¤ ìƒì„±

ì´ ëª¨ë“ˆì€ Co-Scientist ë°©ë²•ë¡ ì„ SAR ë¶„ì„ì— íŠ¹í™”í•˜ì—¬ êµ¬í˜„í•©ë‹ˆë‹¤:
- Phase 1: ë°ì´í„° ì¤€ë¹„ + RAG í†µí•© (ê¸°ì¡´ ì‹œìŠ¤í…œ í™œìš©)
- Phase 2: ì „ë¬¸ê°€ ë¶„ì„ (3ê°œ ë„ë©”ì¸ ì „ë¬¸ê°€ ë…ë¦½ ìƒì„±)
- Phase 3: ì „ë¬¸ê°€ í‰ê°€ (HypothesisEvaluationExpert ê¸°ë°˜ í’ˆì§ˆ í‰ê°€)
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import streamlit as st
from openai import OpenAI
from utils import search_pubmed_for_context, get_activity_cliff_summary

class UnifiedLLMClient:
    """í†µí•© LLM í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str, llm_provider: str = "OpenAI"):
        self.llm_provider = llm_provider
        if llm_provider == "OpenAI":
            self.client = OpenAI(api_key=api_key)
            self.model = "gpt-4o"
        elif llm_provider in ["Gemini", "Google Gemini"]:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel("gemini-2.5-pro")
            self.model = "gemini-2.5-pro"
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ì: {llm_provider}")
    
    def generate_response(self, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
        """í†µí•© ì‘ë‹µ ìƒì„±"""
        if self.llm_provider == "OpenAI":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature
            )
            return response.choices[0].message.content
        elif self.llm_provider in ["Gemini", "Google Gemini"]:
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            response = self.client.generate_content(full_prompt)
            return response.text
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ì: {self.llm_provider}")


class StructuralChemistryExpert:
    """êµ¬ì¡°í™”í•™ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_client: UnifiedLLMClient):
        self.llm_client = llm_client
        self.persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì„ ì„ ì•½í™”í•™ìë¡œ, ë¶„ì êµ¬ì¡°ì™€ ì „ìì  íŠ¹ì„± ë³€í™” ë¶„ì„ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        íŠ¹íˆ ë¶„ì ê¸°í•˜í•™, SMILES êµ¬ì¡° ì°¨ì´ì , í™”í•™ì  ì§ê´€ê³¼ êµ¬ì¡°-ê¸°ëŠ¥ ê´€ê³„ ê·œëª…ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."""
    
    def generate(self, shared_context: Dict[str, Any]) -> Dict[str, Any]:
        """êµ¬ì¡°í™”í•™ ê´€ì ì˜ ê°€ì„¤ ìƒì„±"""
        prompt = self._build_structural_prompt(shared_context)
        hypothesis = self.llm_client.generate_response(self.persona, prompt, temperature=0.7)
        
        return {
            'agent_type': 'structural_chemistry',
            'agent_name': 'êµ¬ì¡°í™”í•™ ì „ë¬¸ê°€',
            'hypothesis': hypothesis,
            'confidence': self._extract_confidence_from_text(hypothesis),
            'key_insights': self._extract_key_insights(hypothesis),
            'reasoning_steps': self._extract_reasoning_steps(hypothesis),
            'timestamp': time.time()
        }
    
    def _build_structural_prompt(self, shared_context: Dict[str, Any]) -> str:
        """êµ¬ì¡°í™”í•™ ì „ë¬¸ê°€ìš© íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± - CoT.md ì§€ì¹¨ ë°˜ì˜"""
        cliff_summary = shared_context['cliff_summary']
        target_name = shared_context['target_name']  # target_name ì¶”ê°€
        high_active = cliff_summary['high_activity_compound']
        low_active = cliff_summary['low_activity_compound']
        metrics = cliff_summary['cliff_metrics']
        prop_diffs = cliff_summary['property_differences']
        
        literature_info = ""
        if shared_context.get('literature_context'):
            lit = shared_context['literature_context']
            literature_info = f"""
            **ì°¸ê³  ë¬¸í—Œ ì •ë³´ (RAG ê²€ìƒ‰ ê²°ê³¼ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©):**
            - ì œëª©: {lit.get('title', 'N/A')}
            - ì´ˆë¡: {lit.get('abstract', 'N/A')[:500]}...
            - PubMed ID: {lit.get('pmid', 'N/A')}
            - í‚¤ì›Œë“œ: {target_name}, êµ¬ì¡°-í™œì„± ê´€ê³„, Activity Cliff
            - ì´ ë¬¸í—Œì„ ì „ë¬¸ê°€ ì§€ì‹ì˜ ê·¼ê±°ë¡œ í™œìš©í•˜ì—¬ ë…¼ë¦¬ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
            """
        
        # Few-Shot ì˜ˆì‹œ (ì‹¤ì œ SAR ì‚¬ë¡€)
        few_shot_example = """
        **Few-Shot ì˜ˆì‹œ - ì „ë¬¸ê°€ ë¶„ì„ ê³¼ì • ì°¸ì¡°:**
        
        [ì˜ˆì‹œ] ë²¤ì¡°ë””ì•„ì œí•€ ìœ ë„ì²´ Activity Cliff ë¶„ì„:
        êµ¬ì¡° A: í´ë¡œë¥´ë””ì•„ì œí­ì‹œë“œ (pKi: 7.2) vs êµ¬ì¡° B: ë””ì•„ì œíŒœ (pKi: 8.9)
        
        1. êµ¬ì¡° ë¹„êµ: AëŠ” N-ì˜¥ì‚¬ì´ë“œ í˜•íƒœ, BëŠ” 7ë²ˆ ìœ„ì¹˜ì— ì—¼ì†Œ ì¹˜í™˜
        2. ë¬¼ë¦¬í™”í•™ì  ì˜í–¥: N-ì˜¥ì‚¬ì´ë“œ ì œê±°ë¡œ ì „ìë°€ë„ ì¦ê°€, ì§€ìš©ì„± í–¥ìƒ (LogP +0.8)
        3. ìƒì²´ ìƒí˜¸ì‘ìš©: GABA ìˆ˜ìš©ì²´ì™€ì˜ ê²°í•© ê¸°í•˜í•™ ê°œì„ , Ï€-Ï€ ìŠ¤íƒí‚¹ ê°•í™”
        4. í™œì„± ë³€í™” ì—°ê²°: ê°œì„ ëœ ë‹¨ë°±ì§ˆ ì í•©ì„±ìœ¼ë¡œ 1.7 pKi ë‹¨ìœ„ í™œì„± ì¦ê°€
        5. ì¶”ê°€ ì‹¤í—˜: ë¶„ì ë„í‚¹ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ê²°í•© ëª¨ë“œ í™•ì¸, ADMET ì˜ˆì¸¡
        
        [ê·€í•˜ì˜ ë¶„ì„ ê³¼ì œ]
        """
        
        return f"""
        ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì„ ì„ ì•½í™”í•™ìì…ë‹ˆë‹¤. SARê³¼ Activity Cliff ë¶„ì„ì—ì„œ ë¶„ì êµ¬ì¡°ì™€ ì „ìì  íŠ¹ì„± ë³€í™” ë¶„ì„ì˜ ì „ë¬¸ê°€ë¡œì„œ, ì‹¤ì œ ì‹ ì•½ ê°œë°œ í˜„ì¥ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì²´ê³„ì  ë¶„ì„ ì ˆì°¨ë¥¼ ë”°ë¼ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê°€ì„¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        {few_shot_example}
        
        **Activity Cliff ë¶„ì„ ëŒ€ìƒ:**
        
        **í™”í•©ë¬¼ ì •ë³´:**
        - ê³ í™œì„± í™”í•©ë¬¼: {high_active['id']} (pKi: {high_active['pki']:.2f})
          SMILES: {high_active['smiles']}
        - ì €í™œì„± í™”í•©ë¬¼: {low_active['id']} (pKi: {low_active['pki']:.2f})
          SMILES: {low_active['smiles']}
        
        **In-Context êµ¬ì¡°ì  íŠ¹ì„± (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©):**
        - Tanimoto ìœ ì‚¬ë„: {metrics['similarity']:.3f}
        - í™œì„±ë„ ì°¨ì´: {metrics['activity_difference']:.2f}
        - êµ¬ì¡°ì  ì°¨ì´ ìœ í˜•: {metrics['structural_difference_type']}
        - ì…ì²´ì´ì„±ì§ˆì²´ ì—¬ë¶€: {metrics['is_stereoisomer_pair']}
        - ë¶„ìëŸ‰ ì°¨ì´: {prop_diffs['mw_diff']:.2f} Da
        - LogP ì°¨ì´: {prop_diffs['logp_diff']:.2f}
        - TPSA ì°¨ì´: {prop_diffs.get('tpsa_diff', 0):.2f} Å²
        
        {literature_info}
        
        **ë‹¨ê³„ë³„ Chain-of-Thought ë¶„ì„ ìˆ˜í–‰:**
        ì‹¤ì œ ì•½í™”í•™ìê°€ ì‚¬ìš©í•˜ëŠ” ë¶„ì„ ì ˆì°¨ë¥¼ ë”°ë¼ ë‹¤ìŒ 5ë‹¨ê³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
        
        1. **êµ¬ì¡° ë¹„êµ**: ë‘ êµ¬ì¡° Aì™€ Bì˜ ì°¨ì´ì ì„ ì •í™•íˆ ì‹ë³„í•˜ì„¸ìš”. SMILES êµ¬ì¡°ë¥¼ ìƒì„¸íˆ ë¹„êµí•˜ì—¬ ì¹˜í™˜ê¸°, ê³ ë¦¬ êµ¬ì¡°, ì…ì²´í™”í•™ì˜ ì •í™•í•œ ë³€í™”ë¥¼ ê¸°ìˆ í•˜ì„¸ìš”.
        
        2. **ë¬¼ë¦¬í™”í•™ì  ì˜í–¥**: ì‹ë³„ëœ ë³€ê²½ì´ ì†Œìˆ˜ì„±(LogP), ìˆ˜ì†Œ ê²°í•© ëŠ¥ë ¥, ì „ì ë¶„í¬, ê·¹ì„± í‘œë©´ì (TPSA)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¶”ë¡ í•˜ì„¸ìš”. ì •ëŸ‰ì  ë³€í™”ê°’ì„ í™œìš©í•˜ì„¸ìš”.
        
        3. **ìƒì²´ ìƒí˜¸ì‘ìš© ê°€ì„¤**: ì´ ë³€ê²½ì´ í‘œì  ë‹¨ë°±ì§ˆ ê²°í•© ì¹œí™”ë„ë‚˜ ëŒ€ì‚¬ ì•ˆì •ì„±ì— ì–´ë–»ê²Œ ì‘ìš©í• ì§€ êµ¬ì²´ì ì¸ ë¶„ì ìˆ˜ì¤€ ë©”ì»¤ë‹ˆì¦˜ì„ ê°€ì„¤ë¡œ ì œì‹œí•˜ì„¸ìš”.
        
        4. **í™œì„± ë³€í™” ì—°ê²°**: ì´ ê°€ì„¤ì´ ê´€ì°°ëœ Activity Cliff ({metrics['activity_difference']:.2f} pKi ë‹¨ìœ„ ì°¨ì´)ë¥¼ ì–´ë–»ê²Œ ì„¤ëª…í•˜ëŠ”ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ì„¸ìš”.
        
        5. **ì¶”ê°€ ì‹¤í—˜ ì œì•ˆ**: ê²€ì¦ì„ ìœ„í•œ ë¶„ì ë„í‚¹, ADMET ì˜ˆì¸¡, ê³„ì‚°í™”í•™ ì‹¤í—˜ ë“± í›„ì† ì‹¤í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”.
        
        **í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ - ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¶„ì„:**
        1. êµ¬ì²´ì  ìˆ˜ì¹˜ ë°ì´í„° í¬í•¨ (LogP, MW, TPSA ë“±)
        2. ì›ì ë‹¨ìœ„ êµ¬ì¡° ì°¨ì´ ëª…ì‹œ (C-N ê²°í•© â†’ C-O ê²°í•© ë“±)
        3. ì •ëŸ‰ì  í™œì„± ì˜ˆì¸¡ ("ëŒ€ëµ 1.5 pKi ë‹¨ìœ„ ê°ì†Œ" ë“±)
        4. êµ¬ì²´ì  ì‹¤í—˜ í”„ë¡œí† ì½œ ("AutoDock4ë¡œ 100íšŒ ë„í‚¹" ë“±)
        5. íŠ¹ì • ë¶„ì ëŒ€ìƒ ì œì‹œ ("ë©”í‹¸ì—ìŠ¤í„° ì¹˜í™˜ì²´" ë“±)
        
        **ê¸ˆì§€ ì‚¬í•­ - ë‹¤ìŒê³¼ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ê¸ˆì§€:**
        - "~ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤", "~ë¡œ ì¶”ì •ëœë‹¤"
        - "ê°€ëŠ¥ì„±ì´ ìˆë‹¤", "ë³´ì¸ë‹¤", "ìƒê°ëœë‹¤"
        - "ì¼ë°˜ì ìœ¼ë¡œ", "ëŒ€ê°œ", "ë³´í†µ"
        
        **ì‹¤ì œ ì œì•½íšŒì‚¬ ìˆ˜ì¤€ì˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ì¦‰ì‹œ í•©ì„± ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì  ê°€ì„¤ì„ ì œì‹œí•˜ì„¸ìš”.**
        
        **ê²°ê³¼ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”):**
        
        ì‹ ë¢°ë„: [êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ê·¼ê±°, ì˜ˆ: 85% - RDKit ê³„ì‚° ê²°ê³¼ì™€ ë¬¸í—Œ ê·¼ê±° ê¸°ë°˜]
        
        í•µì‹¬ ê°€ì„¤: [êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ì¸ 1-2ë¬¸ì¥, ì˜ˆ: "N-ë©”í‹¸ê¸° ì¶”ê°€ë¡œ ì¸í•œ ì…ì²´ì¥ì• ê°€ Asp381ê³¼ì˜ ìˆ˜ì†Œê²°í•©ì„ ë°©í•´í•˜ì—¬ 2.3 pKi ë‹¨ìœ„ í™œì„± ê°ì†Œë¥¼ ì´ˆë˜"]
        
        ìƒì„¸ ë¶„ì„:
        1. êµ¬ì¡° ë¹„êµ: [SMILES êµ¬ì¡°ì˜ ì •í™•í•œ ì°¨ì´ì , ì›ì ë²ˆí˜¸ì™€ ê²°í•© ìœ í˜• ëª…ì‹œ]
        2. ë¬¼ë¦¬í™”í•™ì  ì˜í–¥: [LogP, TPSA, ë¶„ìëŸ‰ ë³€í™”ì˜ êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ì˜ë¯¸]
        3. ìƒì²´ ìƒí˜¸ì‘ìš© ê°€ì„¤: [íŠ¹ì • ì•„ë¯¸ë…¸ì‚° ì”ê¸°ì™€ì˜ ìƒí˜¸ì‘ìš© ë³€í™”, ê²°í•© ì—ë„ˆì§€ ì¶”ì •]
        4. í™œì„± ë³€í™” ì—°ê²°: [ì •ëŸ‰ì  êµ¬ì¡°-í™œì„± ê´€ê³„ ì„¤ëª…]
        5. ì¶”ê°€ ì‹¤í—˜ ì œì•ˆ: [êµ¬ì²´ì  í”„ë¡œí† ì½œê³¼ ì˜ˆìƒ ê²°ê³¼]
        
        ë¶„ì ì„¤ê³„ ì œì•ˆ: [í›„ì† í™”í•©ë¬¼ì˜ êµ¬ì²´ì  êµ¬ì¡° ë³€ê²½ ì „ëµ]
        
        **ì¤‘ìš”: ëª¨ë“  ì„¤ëª…ì€ êµ¬ì²´ì  ìˆ˜ì¹˜, íŠ¹ì • ë¶„ì ë¶€ìœ„, ëª…í™•í•œ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•´ì•¼ í•˜ë©°, '~ì¼ ê²ƒì´ë‹¤', '~ë¡œ ì¶”ì •ëœë‹¤' ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ë³´ë‹¤ëŠ” ê³¼í•™ì  ê·¼ê±°ì— ê¸°ë°˜í•œ í™•ì •ì  ë¶„ì„ì„ ì œì‹œí•˜ì„¸ìš”.**
        """
    
    def _extract_confidence_from_text(self, hypothesis: str) -> float:
        """ê°€ì„¤ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ì‹ ë¢°ë„ ê°’ì„ ì¶”ì¶œ"""
        import re
        
        # "ì‹ ë¢°ë„: XX%" íŒ¨í„´ ì°¾ê¸°
        confidence_match = re.search(r'ì‹ ë¢°ë„:.*?(\d+)%', hypothesis)
        if confidence_match:
            confidence_value = int(confidence_match.group(1))
            return confidence_value / 100.0
        
        # ì˜ì–´ íŒ¨í„´ë„ í™•ì¸
        confidence_match = re.search(r'confidence:.*?(\d+)%', hypothesis, re.IGNORECASE)
        if confidence_match:
            confidence_value = int(confidence_match.group(1))
            return confidence_value / 100.0
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê³„ì‚°ìœ¼ë¡œ fallback
        return self._calculate_confidence_by_keywords(hypothesis)
    
    def _calculate_confidence_by_keywords(self, hypothesis: str) -> float:
        """ê°€ì„¤ì˜ ì‹ ë¢°ë„ ê³„ì‚° (ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±)"""
        confidence_indicators = [
            ('êµ¬ì²´ì ì¸ ë©”ì»¤ë‹ˆì¦˜' in hypothesis or 'mechanism' in hypothesis.lower(), 0.2),
            ('ì‹¤í—˜' in hypothesis or 'experiment' in hypothesis.lower(), 0.15),
            ('ë¬¸í—Œ' in hypothesis or 'literature' in hypothesis.lower(), 0.15),
            ('SMILES' in hypothesis or 'smiles' in hypothesis.lower(), 0.1),
            ('ìˆ˜ì†Œê²°í•©' in hypothesis or 'hydrogen bond' in hypothesis.lower(), 0.1),
            ('ì…ì²´' in hypothesis or 'stereo' in hypothesis.lower(), 0.1),
            ('ë¶„ìëŸ‰' in hypothesis or 'molecular weight' in hypothesis.lower(), 0.1),
            ('í™œì„±' in hypothesis or 'activity' in hypothesis.lower(), 0.1)
        ]
        
        base_confidence = 0.5
        for indicator, weight in confidence_indicators:
            if indicator:
                base_confidence += weight
        
        return min(base_confidence, 1.0)
    
    def _extract_key_insights(self, hypothesis: str) -> List[str]:
        """ê°€ì„¤ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        insights = []
        if 'ì…ì²´' in hypothesis or 'stereo' in hypothesis.lower():
            insights.append("ì…ì²´í™”í•™ì  ì°¨ì´ê°€ í•µì‹¬ ìš”ì¸")
        if 'ìˆ˜ì†Œê²°í•©' in hypothesis or 'hydrogen bond' in hypothesis.lower():
            insights.append("ìˆ˜ì†Œê²°í•© íŒ¨í„´ ë³€í™”")
        if 'ì†Œìˆ˜ì„±' in hypothesis or 'hydrophobic' in hypothesis.lower():
            insights.append("ì†Œìˆ˜ì„± ìƒí˜¸ì‘ìš© ì°¨ì´")
        if not insights:
            insights.append("êµ¬ì¡°ì  ë³€í™”ë¡œ ì¸í•œ í™œì„± ì°¨ì´")
        return insights
    
    def _extract_reasoning_steps(self, hypothesis: str) -> List[str]:
        """ì¶”ë¡  ë‹¨ê³„ ì¶”ì¶œ"""
        # ë²ˆí˜¸ë‚˜ ë‹¨ê³„ë³„ë¡œ ë‚˜ë‰œ ë¶€ë¶„ ì°¾ê¸°
        steps = []
        lines = hypothesis.split('\n')
        current_step = ""
        
        for line in lines:
            line = line.strip()
            if any(marker in line for marker in ['1.', '2.', '3.', '4.', '5.', '**', '###']):
                if current_step:
                    steps.append(current_step.strip())
                current_step = line
            else:
                current_step += " " + line
        
        if current_step:
            steps.append(current_step.strip())
        
        return steps[:5]  # ìµœëŒ€ 5ë‹¨ê³„


class BiomolecularInteractionExpert:
    """ìƒì²´ë¶„ì ìƒí˜¸ì‘ìš© ì „ë¬¸ê°€ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_client: UnifiedLLMClient):
        self.llm_client = llm_client
        self.persona = """ë‹¹ì‹ ì€ ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ ìƒí˜¸ì‘ìš© ë©”ì»¤ë‹ˆì¦˜ ë¶„ì•¼ì˜ ì„¸ê³„ì  ê¶Œìœ„ìì…ë‹ˆë‹¤.
        íƒ€ê²Ÿ ë‹¨ë°±ì§ˆê³¼ì˜ ê²°í•© ë°©ì‹ ë³€í™”, ì•½ë¦¬í•™ì  ê´€ì ê³¼ ìƒë¦¬í™œì„± ë©”ì»¤ë‹ˆì¦˜ ê·œëª…ì„ ì „ë¬¸ìœ¼ë¡œ í•©ë‹ˆë‹¤."""
    
    def generate(self, shared_context: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒì²´ë¶„ì ìƒí˜¸ì‘ìš© ê´€ì ì˜ ê°€ì„¤ ìƒì„±"""
        prompt = self._build_interaction_prompt(shared_context)
        hypothesis = self.llm_client.generate_response(self.persona, prompt, temperature=0.7)
        
        return {
            'agent_type': 'biomolecular_interaction',
            'agent_name': 'ìƒì²´ë¶„ì ìƒí˜¸ì‘ìš© ì „ë¬¸ê°€',
            'hypothesis': hypothesis,
            'confidence': self._extract_confidence_from_text(hypothesis),
            'key_insights': self._extract_key_insights(hypothesis),
            'reasoning_steps': self._extract_reasoning_steps(hypothesis),
            'timestamp': time.time()
        }
    
    def _build_interaction_prompt(self, shared_context: Dict[str, Any]) -> str:
        """ìƒì²´ë¶„ì ìƒí˜¸ì‘ìš© ì „ë¬¸ê°€ìš© íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± - CoT.md ì§€ì¹¨ ë°˜ì˜"""
        cliff_summary = shared_context['cliff_summary']
        target_name = shared_context['target_name']
        high_active = cliff_summary['high_activity_compound']
        low_active = cliff_summary['low_activity_compound']
        metrics = cliff_summary['cliff_metrics']
        prop_diffs = cliff_summary['property_differences']
        
        literature_info = ""
        if shared_context.get('literature_context'):
            lit = shared_context['literature_context']
            literature_info = f"""
            **ì°¸ê³  ë¬¸í—Œ ì •ë³´ (RAG ê²€ìƒ‰ ê²°ê³¼ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©):**
            - ì œëª©: {lit.get('title', 'N/A')}
            - ì´ˆë¡: {lit.get('abstract', 'N/A')[:500]}...
            - PubMed ID: {lit.get('pmid', 'N/A')}
            - í‚¤ì›Œë“œ: {target_name}, êµ¬ì¡°-í™œì„± ê´€ê³„, Activity Cliff
            - ì´ ë¬¸í—Œì„ ì „ë¬¸ê°€ ì§€ì‹ì˜ ê·¼ê±°ë¡œ í™œìš©í•˜ì—¬ ë…¼ë¦¬ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
            """
        
        # Few-Shot ì˜ˆì‹œ (ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ ìƒí˜¸ì‘ìš© ì‚¬ë¡€)
        few_shot_example = """
        **Few-Shot ì˜ˆì‹œ - ì „ë¬¸ê°€ ë¶„ì„ ê³¼ì • ì°¸ì¡°:**
        
        [ì˜ˆì‹œ] EGFR í‚¤ë‚˜ì œ ì–µì œì œ Activity Cliff ë¶„ì„:
        í™”í•©ë¬¼ A: ê²Œí”¼í‹°ë‹ˆë¸Œ (pKi: 7.8) vs í™”í•©ë¬¼ B: ì—˜ë¡œí‹°ë‹ˆë¸Œ (pKi: 8.5)
        
        1. ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ ê²°í•©: í€´ë‚˜ì¡¸ë¦° ì½”ì–´ì˜ 6,7ìœ„ì¹˜ ì¹˜í™˜ê¸° ì°¨ì´ê°€ ATP ê²°í•© í¬ì¼“ê³¼ì˜ ìƒí˜¸ì‘ìš© íŒ¨í„´ ë³€í™”
        2. ìƒí˜¸ì‘ìš© íŒ¨í„´: ì—˜ë¡œí‹°ë‹ˆë¸Œì˜ ì•„ì„¸í‹¸ë Œ ë§ì»¤ê°€ Cys797ê³¼ ìƒˆë¡œìš´ ì†Œìˆ˜ì„± ì ‘ì´‰ í˜•ì„±
        3. ê²°í•© ê¸°í•˜í•™: ì¶”ê°€ ì•„ë¡œë§ˆí‹± ê³ ë¦¬ê°€ DFG ë£¨í”„ì™€ì˜ Ï€-Ï€ ìŠ¤íƒí‚¹ ê°œì„ 
        4. ì•½ë¦¬í•™ì  ë©”ì»¤ë‹ˆì¦˜: í–¥ìƒëœ ê²°í•© ê¸°í•˜í•™ìœ¼ë¡œ 0.7 pKi ë‹¨ìœ„ ì¹œí™”ë„ ì¦ê°€
        5. ADMET ì˜í–¥: CYP3A4 ëŒ€ì‚¬ ì•ˆì •ì„± ê°œì„ , ë°˜ê°ê¸° ì—°ì¥
        
        [ê·€í•˜ì˜ ë¶„ì„ ê³¼ì œ]
        """
        
        return f"""
        ë‹¹ì‹ ì€ ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ ìƒí˜¸ì‘ìš© ë©”ì»¤ë‹ˆì¦˜ ë¶„ì•¼ì˜ ì„¸ê³„ì  ê¶Œìœ„ìì…ë‹ˆë‹¤. íƒ€ê²Ÿ ë‹¨ë°±ì§ˆê³¼ì˜ ê²°í•© ë°©ì‹ ë³€í™”, ì•½ë¦¬í•™ì  ê´€ì ê³¼ ìƒë¦¬í™œì„± ë©”ì»¤ë‹ˆì¦˜ ê·œëª…ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì„ ì„ ì—°êµ¬ìë¡œì„œ, ì‹¤ì œ ì‹ ì•½ ê°œë°œì—ì„œ ì‚¬ìš©ë˜ëŠ” ì²´ê³„ì  ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
        
        {few_shot_example}
        
        **Activity Cliff ë¶„ì„ ëŒ€ìƒ:**
        
        **íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ:** {target_name}
        
        **í™”í•©ë¬¼ ì •ë³´:**
        - ê³ í™œì„± í™”í•©ë¬¼: {high_active['id']} (pKi: {high_active['pki']:.2f})
          SMILES: {high_active['smiles']}
        - ì €í™œì„± í™”í•©ë¬¼: {low_active['id']} (pKi: {low_active['pki']:.2f})
          SMILES: {low_active['smiles']}
        
        **In-Context ìƒí™”í•™ì  íŠ¹ì„± (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©):**
        - í™œì„±ë„ ì°¨ì´: {metrics['activity_difference']:.2f} pKi ë‹¨ìœ„
        - êµ¬ì¡° ìœ ì‚¬ë„: {metrics['similarity']:.3f} (Tanimoto)
        - ë¶„ìëŸ‰ ì°¨ì´: {prop_diffs['mw_diff']:.2f} Da
        - LogP ì°¨ì´: {prop_diffs['logp_diff']:.2f}
        - TPSA ì°¨ì´: {prop_diffs.get('tpsa_diff', 0):.2f} Å²
        - ìˆ˜ì†Œê²°í•© ê³µì—¬ì/ìˆ˜ìš©ì ë³€í™” ì˜ˆìƒ
        
        {literature_info}
        
        **ë‹¨ê³„ë³„ Chain-of-Thought ë¶„ì„ ìˆ˜í–‰:**
        ì‹¤ì œ êµ¬ì¡°ìƒë¬¼í•™ì/ì•½ë¦¬í•™ìê°€ ì‚¬ìš©í•˜ëŠ” ë¶„ì„ ì ˆì°¨ë¥¼ ë”°ë¼ ë‹¤ìŒ 5ë‹¨ê³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
        
        1. **ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ ê²°í•©**: {target_name} í™œì„± ë¶€ìœ„ì™€ì˜ ê²°í•© ë°©ì‹ ì°¨ì´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì„¸ìš”. ì•Œë ¤ì§„ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì •ë³´ì™€ ê²°í•© í¬ì¼“ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.
        
        2. **ìƒí˜¸ì‘ìš© íŒ¨í„´**: ìˆ˜ì†Œê²°í•©, ì†Œìˆ˜ì„± ìƒí˜¸ì‘ìš©, Ï€-Ï€ ìŠ¤íƒí‚¹, ë°˜ë°ë¥´ë°œìŠ¤ í˜ ë“±ì˜ ë³€í™”ê°€ ì–´ë–»ê²Œ í™œì„±ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.
        
        3. **ê²°í•© ê¸°í•˜í•™**: ë¶„ì í˜•íƒœ ë³€í™”ê°€ ë‹¨ë°±ì§ˆ í¬ì¼“ê³¼ì˜ ì…ì²´ì  ì í•©ì„±(shape complementarity)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ 3ì°¨ì› ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”.
        
        4. **ì•½ë¦¬í•™ì  ë©”ì»¤ë‹ˆì¦˜**: ê²°í•© ì¹œí™”ë„ ë³€í™”ê°€ ì–´ë–»ê²Œ ê¸°ëŠ¥ì  í™œì„± ë³€í™”ë¡œ ì´ì–´ì§€ëŠ”ì§€, ì•Œë¡œìŠ¤í…Œë¦­ íš¨ê³¼ë‚˜ ê²°í•© ë™ì—­í•™ì  ìš”ì¸ì„ í¬í•¨í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
        
        5. **ADMET ì˜í–¥**: êµ¬ì¡° ë³€í™”ê°€ ëŒ€ì‚¬ ì•ˆì •ì„±, ì„ íƒì„±, íˆ¬ê³¼ì„± ë“±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ì „ì²´ì ì¸ ì•½ë¬¼ì„±ì— ëŒ€í•œ í•¨ì˜ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
        
        **í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ - êµ¬ì¡°ìƒë¬¼í•™ ì „ë¬¸ê°€ ìˆ˜ì¤€:**
        1. íŠ¹ì • ì•„ë¯¸ë…¸ì‚° ì”ê¸° ë²ˆí˜¸ ëª…ì‹œ (Asp123, Phe456 ë“±)
        2. ê²°í•© ì¹œí™”ë„ ê°’ ê³„ì‚° (Kd, Ki ê°’ ë˜ëŠ” ë¹„ìœ¨)
        3. ìƒí˜¸ì‘ìš© ì—ë„ˆì§€ ì •ëŸ‰í™” (-5.2 kcal/mol ë“±)
        4. ë„í‚¹ ìŠ¤ì½”ì–´ ë¹„êµì™€ RMSD ê°’
        5. ì„ íƒì„± ë¹„ìœ¨ ì˜ˆì¸¡ (vs off-target)
        
        **ê¸ˆì§€ ì‚¬í•­ - ì¼ë°˜ì  ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª… ê¸ˆì§€:**
        - "ìˆ˜ì†Œê²°í•©ì´ ì¤‘ìš”í•˜ë‹¤" â†’ "êµ¬ì²´ì  ìˆ˜ì†Œê²°í•© ê¸¸ì´ì™€ ìœ„ì¹˜"
        - "ì†Œìˆ˜ì„± ìƒí˜¸ì‘ìš©" â†’ "íŠ¹ì • ì†Œìˆ˜ì„± ì”ê¸°ì™€ì˜ ì ‘ì´‰ ë©´ì "
        - "í™œì„±ì´ ê°ì†Œí•œë‹¤" â†’ "IC50 ê°’ 15ë°° ì¦ê°€" ë“±
        
        **ì‹¤ì œ êµ¬ì¡°ìƒë¬¼í•™ ì—°êµ¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì  ë°ì´í„°ì™€ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì‹œí•˜ì„¸ìš”.**
        
        **ê²°ê³¼ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”):**
        
        ì‹ ë¢°ë„: [êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ê·¼ê±°, ì˜ˆ: 78% - ë„í‚¹ ìŠ¤ì½”ì–´ ì°¨ì´ì™€ ê²°í•© ì¹œí™”ë„ ì˜ˆì¸¡ ê¸°ë°˜]
        
        í•µì‹¬ ê°€ì„¤: [êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ì¸ ë©”ì»¤ë‹ˆì¦˜, ì˜ˆ: "Phe256ê³¼ì˜ Ï€-Ï€ ìŠ¤íƒí‚¹ ìƒì‹¤ë¡œ ì¸í•œ ê²°í•© ì¹œí™”ë„ 15ë°° ê°ì†Œê°€ ì£¼ìš” ì›ì¸"]
        
        ìƒì„¸ ë¶„ì„:
        1. ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ ê²°í•©: [íŠ¹ì • ê²°í•© í¬ì¼“, ì”ê¸° ë²ˆí˜¸, ìƒí˜¸ì‘ìš© ìœ í˜• ëª…ì‹œ]
        2. ìƒí˜¸ì‘ìš© íŒ¨í„´: [ìˆ˜ì†Œê²°í•© ê¸¸ì´, ì†Œìˆ˜ì„± ì ‘ì´‰ ë©´ì ì˜ êµ¬ì²´ì  ë³€í™”]
        3. ê²°í•© ê¸°í•˜í•™: [RMSD, ê²°í•©ê°, ë¹„í‹€ë¦¼ê°ì˜ ì •ëŸ‰ì  ë¶„ì„]
        4. ì•½ë¦¬í•™ì  ë©”ì»¤ë‹ˆì¦˜: [Ki/Kd ê°’ ì˜ˆì¸¡, ì„ íƒì„± ë¹„ìœ¨ ê³„ì‚°]
        5. ADMET ì˜í–¥: [CYP ëŒ€ì‚¬, í˜ˆì¥ ë‹¨ë°±ì§ˆ ê²°í•©ë¥ ì˜ êµ¬ì²´ì  ì˜ˆì¸¡]
        
        ë¶„ì ì„¤ê³„ ì œì•ˆ: [íŠ¹ì • ì¹˜í™˜ê¸° ë„ì… ì „ëµê³¼ ì˜ˆìƒ ì¹œí™”ë„ ê°œì„ ]
        
        **ì¤‘ìš”: ê²°í•© ì¹œí™”ë„, ìƒí˜¸ì‘ìš© ì—ë„ˆì§€, íŠ¹ì • ì•„ë¯¸ë…¸ì‚° ì”ê¸° ë²ˆí˜¸ë¥¼ í¬í•¨í•œ ì •ëŸ‰ì  ë¶„ì„ì„ ì œì‹œí•˜ê³ , ì‹¤ì œ êµ¬ì¡°ìƒë¬¼í•™ ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì  ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ëª…í•˜ì„¸ìš”.**
        """
    
    def _extract_confidence_from_text(self, hypothesis: str) -> float:
        """ê°€ì„¤ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ì‹ ë¢°ë„ ê°’ì„ ì¶”ì¶œ"""
        import re
        
        # "ì‹ ë¢°ë„: XX%" íŒ¨í„´ ì°¾ê¸°
        confidence_match = re.search(r'ì‹ ë¢°ë„:.*?(\d+)%', hypothesis)
        if confidence_match:
            confidence_value = int(confidence_match.group(1))
            return confidence_value / 100.0
        
        # ì˜ì–´ íŒ¨í„´ë„ í™•ì¸
        confidence_match = re.search(r'confidence:.*?(\d+)%', hypothesis, re.IGNORECASE)
        if confidence_match:
            confidence_value = int(confidence_match.group(1))
            return confidence_value / 100.0
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê³„ì‚°ìœ¼ë¡œ fallback
        return self._calculate_confidence_by_keywords(hypothesis)
    
    def _calculate_confidence_by_keywords(self, hypothesis: str) -> float:
        """ê°€ì„¤ì˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence_indicators = [
            ('ê²°í•©' in hypothesis or 'binding' in hypothesis.lower(), 0.2),
            ('ë‹¨ë°±ì§ˆ' in hypothesis or 'protein' in hypothesis.lower(), 0.15),
            ('í™œì„±ë¶€ìœ„' in hypothesis or 'active site' in hypothesis.lower(), 0.15),
            ('ìƒí˜¸ì‘ìš©' in hypothesis or 'interaction' in hypothesis.lower(), 0.1),
            ('ì¹œí™”ë„' in hypothesis or 'affinity' in hypothesis.lower(), 0.1),
            ('ì„ íƒì„±' in hypothesis or 'selectivity' in hypothesis.lower(), 0.1),
            ('ëŒ€ì‚¬' in hypothesis or 'metabolism' in hypothesis.lower(), 0.1),
            ('ë„í‚¹' in hypothesis or 'docking' in hypothesis.lower(), 0.1)
        ]
        
        base_confidence = 0.5
        for indicator, weight in confidence_indicators:
            if indicator:
                base_confidence += weight
        
        return min(base_confidence, 1.0)
    
    def _extract_key_insights(self, hypothesis: str) -> List[str]:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = []
        if 'ê²°í•©' in hypothesis or 'binding' in hypothesis.lower():
            insights.append("ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ ê²°í•© ì°¨ì´")
        if 'í™œì„±ë¶€ìœ„' in hypothesis or 'active site' in hypothesis.lower():
            insights.append("í™œì„±ë¶€ìœ„ ìƒí˜¸ì‘ìš© ë³€í™”")
        if 'ì„ íƒì„±' in hypothesis or 'selectivity' in hypothesis.lower():
            insights.append("ì„ íƒì„± ì°¨ì´")
        if not insights:
            insights.append("ìƒì²´ë¶„ì ìƒí˜¸ì‘ìš© ë³€í™”")
        return insights
    
    def _extract_reasoning_steps(self, hypothesis: str) -> List[str]:
        """ì¶”ë¡  ë‹¨ê³„ ì¶”ì¶œ"""
        steps = []
        lines = hypothesis.split('\n')
        current_step = ""
        
        for line in lines:
            line = line.strip()
            if any(marker in line for marker in ['1.', '2.', '3.', '4.', '5.', '**', '###']):
                if current_step:
                    steps.append(current_step.strip())
                current_step = line
            else:
                current_step += " " + line
        
        if current_step:
            steps.append(current_step.strip())
        
        return steps[:5]


class SARIntegrationExpert:
    """SAR í†µí•© ì „ë¬¸ê°€ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_client: UnifiedLLMClient):
        self.llm_client = llm_client
        self.persona = """ë‹¹ì‹ ì€ í™”í•™ì •ë³´í•™ê³¼ ì‹ ì•½ ê°œë°œ íŒŒì´í”„ë¼ì¸ì˜ ì‹¤ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        SAR ë¶„ì„ ìµœì í™”, ì‹ ì•½ ê°œë°œ ì „ëµ ì œì‹œ, ìµœì‹  í™”í•™ì •ë³´í•™ ê¸°ë²• í†µí•©ì´ ì „ë¬¸ ë¶„ì•¼ì…ë‹ˆë‹¤."""
    
    def generate(self, shared_context: Dict[str, Any]) -> Dict[str, Any]:
        """SAR í†µí•© ê´€ì ì˜ ê°€ì„¤ ìƒì„±"""
        prompt = self._build_sar_prompt(shared_context)
        hypothesis = self.llm_client.generate_response(self.persona, prompt, temperature=0.7)
        
        return {
            'agent_type': 'sar_integration',
            'agent_name': 'SAR í†µí•© ì „ë¬¸ê°€',
            'hypothesis': hypothesis,
            'confidence': self._extract_confidence_from_text(hypothesis),
            'key_insights': self._extract_key_insights(hypothesis),
            'reasoning_steps': self._extract_reasoning_steps(hypothesis),
            'timestamp': time.time()
        }
    
    def _build_sar_prompt(self, shared_context: Dict[str, Any]) -> str:
        """SAR í†µí•© ì „ë¬¸ê°€ìš© íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± - CoT.md ì§€ì¹¨ ë°˜ì˜"""
        cliff_summary = shared_context['cliff_summary']
        target_name = shared_context['target_name']
        high_active = cliff_summary['high_activity_compound']
        low_active = cliff_summary['low_activity_compound']
        metrics = cliff_summary['cliff_metrics']
        prop_diffs = cliff_summary['property_differences']
        
        literature_info = ""
        if shared_context.get('literature_context'):
            lit = shared_context['literature_context']
            literature_info = f"""
            **ì°¸ê³  ë¬¸í—Œ ì •ë³´ (RAG ê²€ìƒ‰ ê²°ê³¼ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©):**
            - ì œëª©: {lit.get('title', 'N/A')}
            - ì´ˆë¡: {lit.get('abstract', 'N/A')[:500]}...
            - PubMed ID: {lit.get('pmid', 'N/A')}
            - í‚¤ì›Œë“œ: {target_name}, êµ¬ì¡°-í™œì„± ê´€ê³„, Activity Cliff
            - ì´ ë¬¸í—Œì„ ì „ë¬¸ê°€ ì§€ì‹ì˜ ê·¼ê±°ë¡œ í™œìš©í•˜ì—¬ ë…¼ë¦¬ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
            """
        
        # Few-Shot ì˜ˆì‹œ (SAR ë¶„ì„ ì‚¬ë¡€)
        few_shot_example = """
        **Few-Shot ì˜ˆì‹œ - ì „ë¬¸ê°€ ë¶„ì„ ê³¼ì • ì°¸ì¡°:**
        
        [ì˜ˆì‹œ] ACE ì–µì œì œ ê³„ì—´ SAR ë¶„ì„:
        ì‹œë¦¬ì¦ˆ: ìº…í† í”„ë¦´ â†’ ì—ë‚ ë¼í”„ë¦´ (pKi: 6.5 â†’ 8.2)
        
        1. SAR íŒ¨í„´: í‹°ì˜¬ê¸° â†’ ì¹´ë¥´ë³µì‹¤ê¸° ë³€ê²½ìœ¼ë¡œ 1.7 pKi ë‹¨ìœ„ í™œì„± ì¦ê°€
        2. í™”í•™ì •ë³´í•™ ì¸ì‚¬ì´íŠ¸: ë‚®ì€ Tanimoto ìœ ì‚¬ë„(0.4)ì—ë„ í° í™œì„± ì°¨ì´ëŠ” ì•½ë¬¼ë°œê²¬ì˜ ì „í™˜ì 
        3. ì‹ ì•½ ê°œë°œ ì „ëµ: í”„ë¡œë“œëŸ¬ê·¸ ì „ëµ ë„ì…ìœ¼ë¡œ ADMET íŠ¹ì„± ê°œì„ 
        4. ìµœì í™” ë°©í–¥: ì•„ì—° ê²°í•© ëª¨í‹°í”„ ìµœì í™”ê°€ í•µì‹¬, ì£¼ë³€ ì¹˜í™˜ê¸°ëŠ” ì„ íƒì„± ì¡°ì ˆ
        5. ì˜ˆì¸¡ ëª¨ë¸ë§: ê¸ˆì† ë°°ìœ„ ê²°í•©ì„ ê³ ë ¤í•œ 3D-QSAR ëª¨ë¸ í•„ìš”
        
        [ê·€í•˜ì˜ ë¶„ì„ ê³¼ì œ]
        """
        
        return f"""
        ë‹¹ì‹ ì€ í™”í•™ì •ë³´í•™ê³¼ ì‹ ì•½ ê°œë°œ íŒŒì´í”„ë¼ì¸ì˜ ì‹¤ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. SAR ë¶„ì„ ìµœì í™”, ì‹ ì•½ ê°œë°œ ì „ëµ ì œì‹œ, ìµœì‹  í™”í•™ì •ë³´í•™ ê¸°ë²• í†µí•©ì´ ì „ë¬¸ ë¶„ì•¼ì¸ ì„ ì„ ì—°êµ¬ìë¡œì„œ, ì‹¤ì œ ì œì•½íšŒì‚¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì²´ê³„ì  SAR ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
        
        {few_shot_example}
        
        **Activity Cliff ë¶„ì„ ëŒ€ìƒ:**
        
        **íƒ€ê²Ÿ ë‹¨ë°±ì§ˆ:** {target_name}
        
        **í™”í•©ë¬¼ ì •ë³´:**
        - ê³ í™œì„±: {high_active['id']} (pKi: {high_active['pki']:.2f})
          SMILES: {high_active['smiles']}
        - ì €í™œì„±: {low_active['id']} (pKi: {low_active['pki']:.2f})
          SMILES: {low_active['smiles']}
        
        **In-Context SAR ë©”íŠ¸ë¦­ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©):**
        - Cliff ì ìˆ˜: {metrics.get('cliff_score', 0):.3f}
        - êµ¬ì¡° ìœ ì‚¬ë„: {metrics['similarity']:.3f} (Tanimoto)
        - í™œì„± ì°¨ì´: {metrics['activity_difference']:.2f} pKi ë‹¨ìœ„
        - ê°™ì€ ìŠ¤ìºí´ë“œ: {metrics.get('same_scaffold', 'Unknown')}
        - êµ¬ì¡°ì  ì°¨ì´: {metrics['structural_difference_type']}
        
        **ë¬¼ë¦¬í™”í•™ì  íŠ¹ì„± ì°¨ì´:**
        - ë¶„ìëŸ‰: {prop_diffs['mw_diff']:.2f} Da
        - LogP: {prop_diffs['logp_diff']:.2f} (ì§€ìš©ì„± ë³€í™”)
        - TPSA: {prop_diffs.get('tpsa_diff', 0):.2f} Å² (ê·¹ì„± í‘œë©´ì  ë³€í™”)
        
        {literature_info}
        
        **ë‹¨ê³„ë³„ Chain-of-Thought ë¶„ì„ ìˆ˜í–‰:**
        ì‹¤ì œ í™”í•™ì •ë³´í•™ì/ì‹ ì•½ê°œë°œìê°€ ì‚¬ìš©í•˜ëŠ” ë¶„ì„ ì ˆì°¨ë¥¼ ë”°ë¼ ë‹¤ìŒ 5ë‹¨ê³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
        
        1. **SAR íŒ¨í„´ ë¶„ì„**: ì´ Activity Cliffê°€ ë³´ì—¬ì£¼ëŠ” êµ¬ì¡°-í™œì„± ê´€ê³„ì˜ í•µì‹¬ íŠ¸ë Œë“œë¥¼ ì‹ë³„í•˜ì„¸ìš”. ì–´ë–¤ êµ¬ì¡°ì  ë³€í™”ê°€ í™œì„±ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
        
        2. **í™”í•™ì •ë³´í•™ ì¸ì‚¬ì´íŠ¸**: Tanimoto ìœ ì‚¬ë„ {metrics['similarity']:.3f}ì™€ {metrics['activity_difference']:.2f} pKi ë‹¨ìœ„ í™œì„± ì°¨ì´ì˜ ì¡°í•©ì´ ê°–ëŠ” í™”í•™ì •ë³´í•™ì  ì˜ë¯¸ë¥¼ í•´ì„í•˜ì„¸ìš”. ì´ê²ƒì´ SAR ê³µê°„ì—ì„œ ì˜ë¯¸í•˜ëŠ” ë°”ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
        
        3. **ì‹ ì•½ ê°œë°œ ì „ëµ**: ì´ ê²°ê³¼ê°€ í›„ì† í™”í•©ë¬¼ ì„¤ê³„ì™€ ìµœì í™” ì „ëµì— ì£¼ëŠ” êµ¬ì²´ì ì¸ ì‹œì‚¬ì ì„ ì œì‹œí•˜ì„¸ìš”. Lead optimization ê´€ì ì—ì„œ ìš°ì„ ìˆœìœ„ë¥¼ ì œì•ˆí•˜ì„¸ìš”.
        
        4. **ìµœì í™” ë°©í–¥**: í™œì„± ê°œì„ ì„ ìœ„í•œ êµ¬ì¡° ë³€ê²½ ì „ëµì„ ë¶„ì ì„¤ê³„ ê´€ì ì—ì„œ ì œì•ˆí•˜ì„¸ìš”. ì–´ë–¤ ë¶€ë¶„ì„ ê³ ì •í•˜ê³  ì–´ë–¤ ë¶€ë¶„ì„ ë³€ê²½í•´ì•¼ í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
        
        5. **ì˜ˆì¸¡ ëª¨ë¸ë§**: QSAR/ML ëª¨ë¸ êµ¬ì¶• ì‹œ ì´ Activity Cliff ë°ì´í„°ê°€ ì£¼ëŠ” êµí›ˆê³¼ ëª¨ë¸ ê°œì„  ë°©í–¥ì„ ì œì•ˆí•˜ì„¸ìš”. í”¼ì²˜ ì„ íƒê³¼ ì•Œê³ ë¦¬ì¦˜ ì„ íƒì— ëŒ€í•œ ê°€ì´ë“œë¼ì¸ì„ ì œì‹œí•˜ì„¸ìš”.
        
        **í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ - ì‹ ì•½ê°œë°œ ì „ë¬¸ê°€ ìˆ˜ì¤€:**
        1. ì •ëŸ‰ì  QSAR ëª¨ë¸ ì œì‹œ (RÂ² ê°’, ë°©ì •ì‹ ë“±)
        2. í›„ì† í™”í•©ë¬¼ 3-5ê°œì˜ êµ¬ì²´ì  êµ¬ì¡°ì™€ ì˜ˆìƒ í™œì„±ê°’
        3. í•©ì„± ê°€ëŠ¥ì„±ê³¼ ë¹„ìš© ì¶”ì • (FTE, ë¹„ìš© ë“±)
        4. ì¹˜í™˜ê¸°ë³„ ê¸°ì—¬ë„ ìˆœìœ„ (Hammett ìƒìˆ˜ í™œìš©)
        5. íŠ¹í—ˆ íšŒí”¼ ì „ëµê³¼ ê²½ìŸì‚¬ ë¶„ì„
        
        **ê¸ˆì§€ ì‚¬í•­ - ì¶”ìƒì  ì „ëµ ê¸ˆì§€:**
        - "ìµœì í™”ê°€ í•„ìš”í•˜ë‹¤" â†’ "êµ¬ì²´ì  ìµœì í™” ë‹¨ê³„ì™€ íƒ€ê²Ÿ êµ¬ì¡°"
        - "ë¹„ìŠ·í•œ í™”í•©ë¬¼" â†’ "ì™„ì „í•œ SMILES êµ¬ì¡°ì™€ ì˜ˆìƒ pKi ê°’"
        - "ê°œì„ ì´ ê¸°ëŒ€ëœë‹¤" â†’ "ì •ëŸ‰ì  ê°œì„  ì˜ˆì¸¡ê³¼ ì„±ê³µ í™•ë¥ "
        
        **ì‹¤ì œ ì œì•½íšŒì‚¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì  ë°ì´í„°ì™€ ì „ëµì„ ì œì‹œí•˜ì—¬ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ í”Œëœì„ ì‘ì„±í•˜ì„¸ìš”.**
        
        **ê²°ê³¼ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”):**
        
        ì‹ ë¢°ë„: [êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ê·¼ê±°, ì˜ˆ: 92% - QSAR ëª¨ë¸ ì˜ˆì¸¡ê°’ê³¼ êµ¬ì¡°ì  ìœ ì‚¬ì²´ ë°ì´í„° ì¼ì¹˜]
        
        í•µì‹¬ ê°€ì„¤: [êµ¬ì²´ì  SAR ê´€ê³„, ì˜ˆ: "R2 ìœ„ì¹˜ ì „ìëŒê¸° ì¹˜í™˜ê¸° ë„ì… ì‹œ 0.5 log ë‹¨ìœ„ë‹¹ 1.2 pKi ì¦ê°€ì˜ ì„ í˜• ê´€ê³„"]
        
        ìƒì„¸ ë¶„ì„:
        1. SAR íŒ¨í„´ ë¶„ì„: [Hammett ìƒìˆ˜, ì…ì²´ ë§¤ê°œë³€ìˆ˜ì˜ ì •ëŸ‰ì  ìƒê´€ê´€ê³„]
        2. í™”í•™ì •ë³´í•™ ì¸ì‚¬ì´íŠ¸: [Tanimoto ê³„ìˆ˜ì™€ í™œì„± ì°¨ì´ì˜ ìˆ˜í•™ì  ëª¨ë¸ë§]
        3. ì‹ ì•½ ê°œë°œ ì „ëµ: [ë¦¬ë“œ ìµœì í™” ë‹¨ê³„ë³„ ìš°ì„ ìˆœìœ„ì™€ ì„±ê³µ í™•ë¥ ]
        4. ìµœì í™” ë°©í–¥: [íŠ¹ì • ì¹˜í™˜ê¸°ì˜ ì •ëŸ‰ì  ê¸°ì—¬ë„ì™€ ë‹¤ìŒ í•©ì„± íƒ€ê²Ÿ]
        5. ì˜ˆì¸¡ ëª¨ë¸ë§: [Random Forest/SVM ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ì™€ ì‹ ë¢°êµ¬ê°„]
        
        ë¶„ì ì„¤ê³„ ì œì•ˆ: [êµ¬ì²´ì  êµ¬ì¡°ì‹ê³¼ ì˜ˆìƒ í™œì„±ê°’ì„ í¬í•¨í•œ ì°¨ì„¸ëŒ€ í™”í•©ë¬¼ 3-5ê°œ]
        
        ì‹¤í—˜ ì œì•ˆ: [í•©ì„± ê²½ë¡œ, í™œì„± ì¸¡ì • í”„ë¡œí† ì½œ, ì˜ˆìƒ ë¹„ìš©ê³¼ ê¸°ê°„]
        
        **ì¤‘ìš”: ì •ëŸ‰ì  QSAR ê´€ê³„ì‹, êµ¬ì²´ì  ì¹˜í™˜ê¸° íš¨ê³¼, ì˜ˆì¸¡ í™œì„±ê°’ì„ í¬í•¨í•˜ì—¬ ì‹¤ì œ ì œì•½íšŒì‚¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì˜ êµ¬ì²´ì  ì „ëµì„ ì œì‹œí•˜ì„¸ìš”.**
        """
    
    def _extract_confidence_from_text(self, hypothesis: str) -> float:
        """ê°€ì„¤ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ì‹ ë¢°ë„ ê°’ì„ ì¶”ì¶œ"""
        import re
        
        # "ì‹ ë¢°ë„: XX%" íŒ¨í„´ ì°¾ê¸°
        confidence_match = re.search(r'ì‹ ë¢°ë„:.*?(\d+)%', hypothesis)
        if confidence_match:
            confidence_value = int(confidence_match.group(1))
            return confidence_value / 100.0
        
        # ì˜ì–´ íŒ¨í„´ë„ í™•ì¸
        confidence_match = re.search(r'confidence:.*?(\d+)%', hypothesis, re.IGNORECASE)
        if confidence_match:
            confidence_value = int(confidence_match.group(1))
            return confidence_value / 100.0
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê³„ì‚°ìœ¼ë¡œ fallback
        return self._calculate_confidence_by_keywords(hypothesis)
    
    def _calculate_confidence_by_keywords(self, hypothesis: str) -> float:
        """ê°€ì„¤ì˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence_indicators = [
            ('SAR' in hypothesis or 'sar' in hypothesis.lower(), 0.2),
            ('ìµœì í™”' in hypothesis or 'optimization' in hypothesis.lower(), 0.15),
            ('ì„¤ê³„' in hypothesis or 'design' in hypothesis.lower(), 0.15),
            ('ì˜ˆì¸¡' in hypothesis or 'prediction' in hypothesis.lower(), 0.1),
            ('ëª¨ë¸' in hypothesis or 'model' in hypothesis.lower(), 0.1),
            ('ì „ëµ' in hypothesis or 'strategy' in hypothesis.lower(), 0.1),
            ('íŒ¨í„´' in hypothesis or 'pattern' in hypothesis.lower(), 0.1),
            ('íŠ¸ë Œë“œ' in hypothesis or 'trend' in hypothesis.lower(), 0.1)
        ]
        
        base_confidence = 0.5
        for indicator, weight in confidence_indicators:
            if indicator:
                base_confidence += weight
        
        return min(base_confidence, 1.0)
    
    def _extract_key_insights(self, hypothesis: str) -> List[str]:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = []
        if 'SAR' in hypothesis or 'sar' in hypothesis.lower():
            insights.append("SAR íŒ¨í„´ ì‹ë³„")
        if 'ìµœì í™”' in hypothesis or 'optimization' in hypothesis.lower():
            insights.append("êµ¬ì¡° ìµœì í™” ì „ëµ")
        if 'ì˜ˆì¸¡' in hypothesis or 'prediction' in hypothesis.lower():
            insights.append("ì˜ˆì¸¡ ëª¨ë¸ ê°œì„ ì ")
        if not insights:
            insights.append("SAR íŠ¸ë Œë“œ ë¶„ì„")
        return insights
    
    def _extract_reasoning_steps(self, hypothesis: str) -> List[str]:
        """ì¶”ë¡  ë‹¨ê³„ ì¶”ì¶œ"""
        steps = []
        lines = hypothesis.split('\n')
        current_step = ""
        
        for line in lines:
            line = line.strip()
            if any(marker in line for marker in ['1.', '2.', '3.', '4.', '5.', '**', '###']):
                if current_step:
                    steps.append(current_step.strip())
                current_step = line
            else:
                current_step += " " + line
        
        if current_step:
            steps.append(current_step.strip())
        
        return steps[:5]


class ReflectionAgent:
    """ê°€ì„¤ íƒ€ë‹¹ì„± í‰ê°€ ì—ì´ì „íŠ¸"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o"
        
    def evaluate_hypotheses(self, domain_hypotheses: List[Dict], shared_context: Dict) -> List[Dict]:
        """ê° ê°€ì„¤ì˜ íƒ€ë‹¹ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€"""
        
        st.info("ğŸ¤” **Phase 3: Reflection** - ê°€ì„¤ íƒ€ë‹¹ì„± í‰ê°€ ë° í”¼ë“œë°± ìƒì„±ì¤‘...")
        
        evaluation_results = []
        
        for i, hypothesis in enumerate(domain_hypotheses):
            with st.spinner(f"{hypothesis['agent_name']} ê°€ì„¤ í‰ê°€ ì¤‘..."):
                evaluation_prompt = self._build_evaluation_prompt(hypothesis, shared_context)
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ê³¼í•™ì  ê°€ì„¤ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ê±´ì„¤ì ì¸ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                        {"role": "user", "content": evaluation_prompt}
                    ],
                    temperature=0.3
                )
                
                evaluation_text = response.choices[0].message.content
                
                # í‰ê°€ ì ìˆ˜ íŒŒì‹±
                scores = self._parse_evaluation_scores(evaluation_text)
                
                result = {
                    'hypothesis_id': i,
                    'agent_name': hypothesis['agent_name'],
                    'original_hypothesis': hypothesis,
                    'evaluation_text': evaluation_text,
                    'scores': scores,
                    'feedback': self._extract_feedback(evaluation_text),
                    'strengths': self._extract_strengths(evaluation_text),
                    'weaknesses': self._extract_weaknesses(evaluation_text),
                    'timestamp': time.time()
                }
                
                evaluation_results.append(result)
                
                # í‰ê°€ ê²°ê³¼ ì¦‰ì‹œ í‘œì‹œ
                self._display_reflection_result(result)
        
        return evaluation_results
    
    def _build_evaluation_prompt(self, hypothesis: Dict, shared_context: Dict) -> str:
        """í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        return f"""
        **ê°€ì„¤ í‰ê°€ ìš”ì²­:**
        
        **ì „ë¬¸ê°€:** {hypothesis['agent_name']}
        **ê°€ì„¤ ë‚´ìš©:**
        {hypothesis['hypothesis']}
        
        **ì›ë³¸ ì‹ ë¢°ë„:** {hypothesis['confidence']:.0%}
        
        **í‰ê°€ ìš”ì²­:**
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì´ ê°€ì„¤ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        1. **ê³¼í•™ì  ì—„ë°€ì„±** (Scientific Rigor): ë…¼ë¦¬ì  ì¼ê´€ì„±, ê³¼í•™ì  ê·¼ê±°
        2. **ì¦ê±° í†µí•©** (Evidence Integration): ë°ì´í„°ì™€ ë¬¸í—Œ í™œìš©ë„
        3. **ì‹¤ìš©ì„±** (Practical Applicability): ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±
        4. **í˜ì‹ ì„±** (Innovation): ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ ì œê³µ
        
        **í‰ê°€ í˜•ì‹:**
        ì ìˆ˜: [ê° ê¸°ì¤€ë³„ 0-100ì ]
        ê°•ì : [2-3ê°œ í•­ëª©]
        ì•½ì : [1-2ê°œ í•­ëª©] 
        ê°œì„  ì œì•ˆ: [êµ¬ì²´ì  í”¼ë“œë°±]
        ì´í‰: [ì¢…í•© í‰ê°€]
        
        ê°ê´€ì ì´ê³  ê±´ì„¤ì ì¸ í‰ê°€ë¥¼ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
        """
    
    def _parse_evaluation_scores(self, evaluation_text: str) -> Dict[str, float]:
        """í‰ê°€ í…ìŠ¤íŠ¸ì—ì„œ ì ìˆ˜ ì¶”ì¶œ - ê°œì„ ëœ íŒŒì‹± ë¡œì§"""
        # ê¸°ë³¸ê°’ì„ í•©ë¦¬ì ì¸ ë²”ìœ„ë¡œ ì„¤ì •
        scores = {
            'scientific_rigor': 75.0,
            'evidence_integration': 75.0,
            'practical_applicability': 75.0,
            'innovation': 75.0
        }
        
        # ë” ì •í™•í•œ ì ìˆ˜ ì¶”ì¶œì„ ìœ„í•œ ê°œì„ ëœ ë¡œì§
        lines = evaluation_text.split('\n')
        for line in lines:
            line_lower = line.lower()
            # ì ìˆ˜ íŒ¨í„´ ì°¾ê¸°: "ì—„ë°€ì„±: 85ì " ë˜ëŠ” "Scientific Rigor: 85"
            if any(keyword in line_lower for keyword in ['ì—„ë°€ì„±', 'rigor', 'ê³¼í•™ì ']):
                score = self._extract_score_from_line(line)
                if score is not None and 0 <= score <= 100:
                    scores['scientific_rigor'] = score
            elif any(keyword in line_lower for keyword in ['ì¦ê±°', 'evidence', 'í†µí•©']):
                score = self._extract_score_from_line(line)
                if score is not None and 0 <= score <= 100:
                    scores['evidence_integration'] = score
            elif any(keyword in line_lower for keyword in ['ì‹¤ìš©', 'practical', 'ì ìš©']):
                score = self._extract_score_from_line(line)
                if score is not None and 0 <= score <= 100:
                    scores['practical_applicability'] = score
            elif any(keyword in line_lower for keyword in ['í˜ì‹ ', 'innovation', 'ì°½ì˜']):
                score = self._extract_score_from_line(line)
                if score is not None and 0 <= score <= 100:
                    scores['innovation'] = score
        
        # ëª¨ë“  ì ìˆ˜ê°€ ìœ íš¨í•œ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        for key, value in scores.items():
            if not isinstance(value, (int, float)) or value < 0 or value > 100:
                scores[key] = 75.0  # ì•ˆì „í•œ ê¸°ë³¸ê°’
        
        return scores
    
    def _extract_score_from_line(self, line: str) -> Optional[float]:
        """ë¼ì¸ì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        import re
        # 0-100 ë²”ìœ„ì˜ ìˆ«ì ì°¾ê¸°
        matches = re.findall(r'\b(\d{1,3})\b', line)
        for match in matches:
            score = float(match)
            if 0 <= score <= 100:
                return score
        return None
    
    def _extract_feedback(self, evaluation_text: str) -> List[str]:
        """í”¼ë“œë°± ì¶”ì¶œ"""
        feedback = []
        lines = evaluation_text.split('\n')
        in_feedback_section = False
        
        for line in lines:
            line = line.strip()
            if 'ê°œì„ ' in line or 'feedback' in line.lower() or 'ì œì•ˆ' in line:
                in_feedback_section = True
                continue
            elif in_feedback_section and line and not line.startswith('**'):
                feedback.append(line)
            elif in_feedback_section and line.startswith('**'):
                break
        
        return feedback[:3]  # ìµœëŒ€ 3ê°œ
    
    def _extract_strengths(self, evaluation_text: str) -> List[str]:
        """ê°•ì  ì¶”ì¶œ"""
        strengths = []
        lines = evaluation_text.split('\n')
        in_strengths_section = False
        
        for line in lines:
            line = line.strip()
            if 'ê°•ì ' in line or 'strength' in line.lower():
                in_strengths_section = True
                continue
            elif in_strengths_section and line and not line.startswith('**'):
                strengths.append(line)
            elif in_strengths_section and line.startswith('**'):
                break
        
        return strengths[:3]  # ìµœëŒ€ 3ê°œ
    
    def _extract_weaknesses(self, evaluation_text: str) -> List[str]:
        """ì•½ì  ì¶”ì¶œ"""
        weaknesses = []
        lines = evaluation_text.split('\n')
        in_weaknesses_section = False
        
        for line in lines:
            line = line.strip()
            if 'ì•½ì ' in line or 'weakness' in line.lower():
                in_weaknesses_section = True
                continue
            elif in_weaknesses_section and line and not line.startswith('**'):
                weaknesses.append(line)
            elif in_weaknesses_section and line.startswith('**'):
                break
        
        return weaknesses[:2]  # ìµœëŒ€ 2ê°œ
    
    def _display_reflection_result(self, result: Dict):
        """í‰ê°€ ê²°ê³¼ í‘œì‹œ"""
        with st.expander(f"ğŸ“ {result['agent_name']} í‰ê°€ ê²°ê³¼", expanded=False):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.write("**í‰ê°€ ìš”ì•½:**")
                st.write(result['evaluation_text'][:200] + "..." if len(result['evaluation_text']) > 200 else result['evaluation_text'])
                
                if result['strengths']:
                    st.write("**ì£¼ìš” ê°•ì :**")
                    for strength in result['strengths']:
                        st.write(f"â€¢ {strength}")
                        
                if result['weaknesses']:
                    st.write("**ê°œì„ ì :**")
                    for weakness in result['weaknesses']:
                        st.write(f"â€¢ {weakness}")
            
            with col2:
                st.write("**í‰ê°€ ì ìˆ˜:**")
                avg_score = sum(result['scores'].values()) / len(result['scores'])
                st.metric("ì¢…í•© ì ìˆ˜", f"{avg_score:.1f}/100")
                
                for criterion, score in result['scores'].items():
                    criterion_kr = {
                        'scientific_rigor': 'ê³¼í•™ì  ì—„ë°€ì„±',
                        'evidence_integration': 'ì¦ê±° í†µí•©',
                        'practical_applicability': 'ì‹¤ìš©ì„±',
                        'innovation': 'í˜ì‹ ì„±'
                    }.get(criterion, criterion)
                    st.metric(criterion_kr, f"{score:.1f}")


class EloRankingAgent:
    """Elo ì‹œìŠ¤í…œ ê¸°ë°˜ ìˆœìœ„ ë§¤ê¹€ ì—ì´ì „íŠ¸"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o"
        self.initial_elo = 1500  # ì´ˆê¸° Elo ì ìˆ˜
        self.k_factor = 32  # Elo ì—…ë°ì´íŠ¸ ê³„ìˆ˜
        
    async def perform_elo_comparisons(self, reflection_results: List[Dict], criteria_weights: Dict = None) -> Tuple[List[Dict], float]:
        """Elo ì‹œìŠ¤í…œìœ¼ë¡œ ê°€ì„¤ ê°„ ìƒëŒ€ì  ìš°ìˆ˜ì„± í‰ê°€"""
        
        if criteria_weights is None:
            criteria_weights = {
                'logical_consistency': 0.4,
                'research_relevance': 0.3,
                'innovation': 0.3
            }
        
        st.info("ğŸ† **Phase 4: Ranking** - Elo ì‹œìŠ¤í…œìœ¼ë¡œ ê°€ì„¤ ìˆœìœ„ ë§¤ê¹€ì¤‘...")
        
        # ì´ˆê¸° Elo ì ìˆ˜ ì„¤ì •
        elo_scores = {i: self.initial_elo for i in range(len(reflection_results))}
        
        comparison_results = []
        total_comparisons = len(reflection_results) * (len(reflection_results) - 1) // 2
        current_comparison = 0
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # ëª¨ë“  ê°€ì„¤ ìŒì— ëŒ€í•´ ë¹„êµ ìˆ˜í–‰
        for i in range(len(reflection_results)):
            for j in range(i + 1, len(reflection_results)):
                current_comparison += 1
                progress = current_comparison / total_comparisons
                progress_bar.progress(progress)
                status_text.text(f"Elo ë¹„êµ ì§„í–‰ì¤‘... ({current_comparison}/{total_comparisons})")
                
                hypothesis_a = reflection_results[i]
                hypothesis_b = reflection_results[j]
                
                # ìŒë³„ ë¹„êµ ìˆ˜í–‰
                comparison_result = await self._compare_hypotheses_pair(
                    hypothesis_a, hypothesis_b, criteria_weights
                )
                
                # Elo ì ìˆ˜ ì—…ë°ì´íŠ¸
                old_elo_a, old_elo_b = elo_scores[i], elo_scores[j]
                new_elo_a, new_elo_b = self._update_elo_scores(
                    old_elo_a, old_elo_b, comparison_result
                )
                
                elo_scores[i] = new_elo_a
                elo_scores[j] = new_elo_b
                
                # ë¹„êµ ê³¼ì • ì‹œê°í™”
                self._display_elo_comparison(hypothesis_a, hypothesis_b, comparison_result, 
                                           new_elo_a, new_elo_b, old_elo_a, old_elo_b)
                
                comparison_results.append({
                    'pair': (i, j),
                    'winner': comparison_result['winner'],
                    'reasoning': comparison_result['reasoning'],
                    'confidence': comparison_result['confidence'],
                    'elo_change': (new_elo_a - old_elo_a, new_elo_b - old_elo_b)
                })
        
        # ìµœì¢… ìˆœìœ„ ë§¤ê¹€
        ranked_hypotheses = self._rank_by_elo_scores(reflection_results, elo_scores)
        consensus_score = self._calculate_consensus_score(elo_scores)
        
        # ìµœì¢… Elo ìˆœìœ„ í‘œì‹œ
        self._display_final_elo_ranking(ranked_hypotheses, elo_scores, consensus_score)
        
        return ranked_hypotheses, consensus_score
    
    async def _compare_hypotheses_pair(self, hyp_a: Dict, hyp_b: Dict, criteria_weights: Dict) -> Dict:
        """ë‘ ê°€ì„¤ì„ ì§ì ‘ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ê°€ì„¤ ì„ ì •"""
        
        comparison_prompt = f"""
        **ê°€ì„¤ ë¹„êµ ìš”ì²­:**
        
        ë‹¤ìŒ ë‘ ê°€ì„¤ì„ ê°ê´€ì ìœ¼ë¡œ ë¹„êµí•˜ê³  ì–´ëŠ ê²ƒì´ ë” ìš°ìˆ˜í•œì§€ íŒë‹¨í•˜ì„¸ìš”.
        
        **í‰ê°€ ê¸°ì¤€ ê°€ì¤‘ì¹˜:**
        - ë…¼ë¦¬ì  ì¼ê´€ì„±: {criteria_weights['logical_consistency']:.1f}
        - ê¸°ì¡´ ì—°êµ¬ ì—°ê´€ì„±: {criteria_weights['research_relevance']:.1f}
        - í˜ì‹ ì„±: {criteria_weights['innovation']:.1f}
        
        **ê°€ì„¤ A ({hyp_a['agent_name']}):**
        {hyp_a['original_hypothesis']['hypothesis']}
        
        í‰ê°€ ì ìˆ˜: {hyp_a['scores']}
        
        **ê°€ì„¤ B ({hyp_b['agent_name']}):**
        {hyp_b['original_hypothesis']['hypothesis']}
        
        í‰ê°€ ì ìˆ˜: {hyp_b['scores']}
        
        **ë¹„êµ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:**
        {{
            "winner": "A" ë˜ëŠ” "B",
            "confidence": 0.5-1.0,
            "reasoning": "êµ¬ì²´ì ì¸ ë¹„êµ ì´ìœ  (100ì ì´ë‚´)",
            "criteria_analysis": {{
                "logical_consistency": "A ë˜ëŠ” Bê°€ ìš°ìˆ˜í•œ ì´ìœ ",
                "research_relevance": "A ë˜ëŠ” Bê°€ ìš°ìˆ˜í•œ ì´ìœ ", 
                "innovation": "A ë˜ëŠ” Bê°€ ìš°ìˆ˜í•œ ì´ìœ "
            }}
        }}
        """
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê³¼í•™ì  ê°€ì„¤ ë¹„êµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì¼ê´€ëœ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": comparison_prompt}
            ],
            temperature=0.2
        )
        
        try:
            response_text = response.choices[0].message.content.strip()
            
            # JSON ë¸”ë¡ì„ ì°¾ì•„ì„œ ì¶”ì¶œ
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                result = json.loads(json_text)
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ë³´ì™„
                if "winner" not in result:
                    result["winner"] = "A"
                if "confidence" not in result:
                    result["confidence"] = 0.6
                if "reasoning" not in result:
                    result["reasoning"] = "ë¹„êµ ë¶„ì„ ì™„ë£Œ"
                if "criteria_analysis" not in result:
                    result["criteria_analysis"] = {}
            else:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ íœ´ë¦¬ìŠ¤í‹± ë¶„ì„
            response_text = response.choices[0].message.content.lower()
            
            # A ë˜ëŠ” B ìŠ¹ì ê²°ì •
            winner = "B"  # ê¸°ë³¸ê°’
            if "ê°€ì„¤ a" in response_text and "ìš°ìˆ˜" in response_text:
                winner = "A"
            elif "ê°€ì„¤ b" in response_text and "ìš°ìˆ˜" in response_text:
                winner = "B"
            
            # ì‹ ë¢°ë„ ì¶”ì •
            confidence = 0.7
            if "í™•ì‹¤" in response_text or "ëª…í™•" in response_text:
                confidence = 0.8
            elif "ì• ë§¤" in response_text or "ìœ ì‚¬" in response_text:
                confidence = 0.6
            
            result = {
                "winner": winner,
                "confidence": confidence,
                "reasoning": f"ì‘ë‹µ ê¸°ë°˜ ë¶„ì„: {response.choices[0].message.content[:100]}..." if hasattr(response.choices[0].message, 'content') else "ë¶„ì„ ì™„ë£Œ",
                "criteria_analysis": {
                    "logical_consistency": f"ê°€ì„¤ {winner}ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ ë” ì¼ê´€ì„± ìˆìŒ",
                    "research_relevance": f"ê°€ì„¤ {winner}ê°€ ì—°êµ¬ì™€ ë” ê´€ë ¨ì„± ë†’ìŒ",
                    "innovation": f"ê°€ì„¤ {winner}ê°€ ë” í˜ì‹ ì  ê´€ì  ì œì‹œ"
                }
            }
        
        return result
    
    def _update_elo_scores(self, elo_a: float, elo_b: float, comparison_result: Dict) -> Tuple[float, float]:
        """Elo ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        # ì˜ˆìƒ ìŠ¹ë¥  ê³„ì‚°
        expected_a = 1 / (1 + 10 ** ((elo_b - elo_a) / 400))
        expected_b = 1 - expected_a
        
        # ì‹¤ì œ ê²°ê³¼ (winnerì— ë”°ë¼)
        if comparison_result['winner'] == 'A':
            actual_a, actual_b = 1, 0
        else:
            actual_a, actual_b = 0, 1
        
        # ì‹ ë¢°ë„ë¥¼ ë°˜ì˜í•œ K-factor ì¡°ì •
        confidence = comparison_result.get('confidence', 0.6)
        adjusted_k = self.k_factor * confidence
        
        # ìƒˆë¡œìš´ Elo ì ìˆ˜ ê³„ì‚°
        new_elo_a = elo_a + adjusted_k * (actual_a - expected_a)
        new_elo_b = elo_b + adjusted_k * (actual_b - expected_b)
        
        return new_elo_a, new_elo_b
    
    def _rank_by_elo_scores(self, reflection_results: List[Dict], elo_scores: Dict) -> List[Dict]:
        """Elo ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì„¤ ìˆœìœ„ ë§¤ê¹€"""
        # (index, elo_score) íŠœí”Œë¡œ ë³€í™˜ í›„ ì •ë ¬
        sorted_indices = sorted(elo_scores.items(), key=lambda x: x[1], reverse=True)
        
        ranked_hypotheses = []
        for rank, (index, elo_score) in enumerate(sorted_indices):
            hypothesis = reflection_results[index].copy()
            hypothesis['rank'] = rank + 1
            hypothesis['elo_score'] = elo_score
            hypothesis['elo_rating'] = self._get_elo_rating(elo_score)
            ranked_hypotheses.append(hypothesis)
        
        return ranked_hypotheses
    
    def _get_elo_rating(self, elo_score: float) -> str:
        """Elo ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if elo_score >= 1700:
            return "Sê¸‰ (íƒì›”)"
        elif elo_score >= 1600:
            return "Aê¸‰ (ìš°ìˆ˜)"
        elif elo_score >= 1500:
            return "Bê¸‰ (í‰ê· )"
        elif elo_score >= 1400:
            return "Cê¸‰ (ë³´í†µ)"
        else:
            return "Dê¸‰ (ë¯¸í¡)"
    
    def _calculate_consensus_score(self, elo_scores: Dict) -> float:
        """Elo ì ìˆ˜ ë¶„ì‚°ì„ ë°”íƒ•ìœ¼ë¡œ í•©ì˜ë„ ê³„ì‚°"""
        scores = list(elo_scores.values())
        if len(scores) <= 1:
            return 1.0
        
        mean_score = sum(scores) / len(scores)
        variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
        std_dev = variance ** 0.5
        
        # í‘œì¤€í¸ì°¨ë¥¼ 0-1 ë²”ìœ„ì˜ í•©ì˜ë„ë¡œ ë³€í™˜ (ë‚®ì€ í‘œì¤€í¸ì°¨ = ë†’ì€ í•©ì˜ë„)
        # í‘œì¤€í¸ì°¨ 100 ì´ìƒì€ í•©ì˜ë„ 0, 0ì€ í•©ì˜ë„ 1ë¡œ ì„¤ì •
        consensus_score = max(0, 1 - (std_dev / 100))
        
        return consensus_score
    
    def _display_elo_comparison(self, hyp_a: Dict, hyp_b: Dict, comparison_result: Dict, 
                               elo_a: float, elo_b: float, old_elo_a: float, old_elo_b: float):
        """Elo ë¹„êµ ê³¼ì • í‘œì‹œ"""
        with st.expander(f"âš”ï¸ Elo ë¹„êµ: {hyp_a['agent_name']} vs {hyp_b['agent_name']}", expanded=False):
            col1, col2, col3 = st.columns([1, 1, 1])
            
            with col1:
                st.write("**ê°€ì„¤ A**")
                st.write(hyp_a['agent_name'])
                elo_change_a = elo_a - old_elo_a
                st.metric("Elo ì ìˆ˜", f"{elo_a:.0f}", f"{elo_change_a:+.0f}")
                
            with col2:
                st.write("**ë¹„êµ ê²°ê³¼**")
                winner_name = hyp_a['agent_name'] if comparison_result['winner'] == 'A' else hyp_b['agent_name']
                st.success(f"ğŸ† {winner_name}")
                st.metric("ì‹ ë¢°ë„", f"{comparison_result.get('confidence', 0.6):.0%}")
                
            with col3:
                st.write("**ê°€ì„¤ B**")
                st.write(hyp_b['agent_name'])
                elo_change_b = elo_b - old_elo_b
                st.metric("Elo ì ìˆ˜", f"{elo_b:.0f}", f"{elo_change_b:+.0f}")
            
            st.write("**ë¹„êµ ê·¼ê±°:**")
            st.write(comparison_result['reasoning'])
    
    def _display_final_elo_ranking(self, ranked_hypotheses: List[Dict], elo_scores: Dict, consensus_score: float):
        """ìµœì¢… Elo ìˆœìœ„ í‘œì‹œ"""
        with st.container():
            st.markdown("### ğŸ† ìµœì¢… Elo ìˆœìœ„")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                for i, hypothesis in enumerate(ranked_hypotheses):
                    rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else f"{i+1}."
                    st.write(f"{rank_emoji} **{hypothesis['agent_name']}** - Elo: {hypothesis['elo_score']:.0f} ({hypothesis['elo_rating']})")
            
            with col2:
                st.metric("ì—ì´ì „íŠ¸ ê°„ í•©ì˜ë„", f"{consensus_score:.2f}", 
                         "ë†’ìŒ" if consensus_score >= 0.8 else "ë³´í†µ" if consensus_score >= 0.6 else "ë‚®ìŒ")
                
                avg_elo = sum(elo_scores.values()) / len(elo_scores)
                st.metric("í‰ê·  Elo ì ìˆ˜", f"{avg_elo:.0f}")


class EvolutionAgent:
    """Self-Play ê°€ì„¤ ê°œì„  ì—ì´ì „íŠ¸"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o"
        
    async def self_play_improvement(self, ranked_hypotheses: List[Dict], consensus_score: float, shared_context: Dict) -> List[Dict]:
        """Self-Play ë…¼ìŸì„ í†µí•œ ê°€ì„¤ ê°œì„ """
        
        if consensus_score >= 0.8:
            st.info("âœ… **Phase 5A: Evolution** - ì—ì´ì „íŠ¸ ê°„ í•©ì˜ë„ê°€ ë†’ì•„ Evolution ë‹¨ê³„ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
            st.metric("í•©ì˜ë„ ì ìˆ˜", f"{consensus_score:.2f}", "ë†’ìŒ (â‰¥0.8)")
            return ranked_hypotheses
            
        st.info("âš”ï¸ **Phase 5A: Evolution** - Self-Play ë…¼ìŸì„ í†µí•œ ê°€ì„¤ ê°œì„  ì§„í–‰ì¤‘...")
        st.metric("í•©ì˜ë„ ì ìˆ˜", f"{consensus_score:.2f}", "ë‚®ìŒ (<0.8)")
        
        improved_hypotheses = []
        
        # ìƒìœ„ 2ê°œ ê°€ì„¤ì— ëŒ€í•´ì„œë§Œ Self-Play ì§„í–‰ (ì‹œê°„ ì ˆì•½)
        for i, hypothesis in enumerate(ranked_hypotheses[:2]):
            st.markdown(f"### ğŸ¥Š ê°€ì„¤ {i+1} Self-Play ë…¼ìŸ")
            
            # 1ë‹¨ê³„: ëŒ€ì•ˆ ê°€ì„¤ ìƒì„±
            st.write("**1ë‹¨ê³„: ëŒ€ì•ˆ ê°€ì„¤ ìƒì„±**")
            with st.spinner("ëŒ€ì•ˆ ê°€ì„¤ ìƒì„± ì¤‘..."):
                alternative = await self._generate_alternative_hypothesis(hypothesis, shared_context)
            
            col1, col2 = st.columns(2)
            with col1:
                st.write("**ì›ë³¸ ê°€ì„¤**")
                st.write(hypothesis['original_hypothesis']['hypothesis'][:200] + "...")
            with col2:
                st.write("**ëŒ€ì•ˆ ê°€ì„¤**")
                st.write(alternative['hypothesis'][:200] + "...")
            
            # 2ë‹¨ê³„: 3ë¼ìš´ë“œ ë…¼ìŸ ì‹œë®¬ë ˆì´ì…˜
            st.write("**2ë‹¨ê³„: ë…¼ìŸ ì‹œë®¬ë ˆì´ì…˜ (3ë¼ìš´ë“œ)**")
            debate_results = []
            
            for round_num in range(1, 4):
                st.write(f"**ë¼ìš´ë“œ {round_num}**")
                
                with st.spinner(f"ë¼ìš´ë“œ {round_num} ë…¼ìŸ ì§„í–‰ ì¤‘..."):
                    debate_round = await self._simulate_debate_round(
                        hypothesis, alternative, shared_context, round_num
                    )
                
                debate_results.append(debate_round)
                
                # ë¼ìš´ë“œë³„ ê²°ê³¼ í‘œì‹œ
                self._display_debate_round_result(debate_round, round_num)
            
            # 3ë‹¨ê³„: ê°œì„ ëœ ê°€ì„¤ í•©ì„±
            st.write("**3ë‹¨ê³„: ê°œì„ ëœ ê°€ì„¤ í•©ì„±**")
            with st.spinner("ê°œì„ ëœ ê°€ì„¤ í•©ì„± ì¤‘..."):
                improved = await self._synthesize_improved_hypothesis(
                    hypothesis, alternative, debate_results, shared_context
                )
            
            # ê°œì„  ê²°ê³¼ í‘œì‹œ
            self._display_improvement_result(hypothesis, improved)
            
            improved_hypotheses.append(improved)
        
        # ê°œì„ ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ê°€ì„¤ë“¤ë„ í¬í•¨
        for hypothesis in ranked_hypotheses[2:]:
            improved_hypotheses.append(hypothesis)
        
        return improved_hypotheses
    
    async def _generate_alternative_hypothesis(self, original_hypothesis: Dict, shared_context: Dict) -> Dict:
        """ì›ë³¸ ê°€ì„¤ì˜ ëŒ€ì•ˆ ìƒì„±"""
        
        prompt = f"""
        **ëŒ€ì•ˆ ê°€ì„¤ ìƒì„± ìš”ì²­:**
        
        ë‹¤ìŒ ì›ë³¸ ê°€ì„¤ì— ëŒ€í•œ ê±´ì„¤ì ì¸ ëŒ€ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”:
        
        **ì›ë³¸ ê°€ì„¤ ({original_hypothesis['agent_name']}):**
        {original_hypothesis['original_hypothesis']['hypothesis']}
        
        **Activity Cliff ë§¥ë½:**
        {shared_context['cliff_summary']}
        
        **ëŒ€ì•ˆ ìƒì„± ì§€ì¹¨:**
        1. ì›ë³¸ ê°€ì„¤ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ìœ ì§€í•˜ë˜, ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ë©”ì»¤ë‹ˆì¦˜ ì œì‹œ
        2. ë™ì¼í•œ ë°ì´í„°ë¥¼ ë‹¤ë¥´ê²Œ í•´ì„í•  ìˆ˜ ìˆëŠ” ê³¼í•™ì  ê·¼ê±° ì œê³µ
        3. ì›ë³¸ë³´ë‹¤ ë” êµ¬ì²´ì ì´ê±°ë‚˜ í¬ê´„ì ì¸ ì„¤ëª… ì‹œë„
        4. ì‹¤í—˜ì  ê²€ì¦ ë°©ë²•ë„ í•¨ê»˜ ì œì•ˆ
        
        **ê²°ê³¼ í˜•ì‹:**
        - ëŒ€ì•ˆì˜ í•µì‹¬ ì°¨ì´ì : [ì›ë³¸ê³¼ì˜ ì£¼ìš” ì°¨ì´]
        - ëŒ€ì•ˆ ê°€ì„¤: [ìƒì„¸í•œ ëŒ€ì•ˆ ì„¤ëª…]
        - ìš°ìˆ˜ì„± ì£¼ì¥: [ì™œ ì´ ëŒ€ì•ˆì´ ê³ ë ¤ë  ë§Œí•œê°€]
        """
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ë¹„íŒì  ì‚¬ê³ ë ¥ì„ ê°€ì§„ ê³¼í•™ìì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8
        )
        
        return {
            'hypothesis': response.choices[0].message.content,
            'type': 'alternative',
            'source': 'evolution_agent',
            'timestamp': time.time()
        }
    
    async def _simulate_debate_round(self, original: Dict, alternative: Dict, shared_context: Dict, round_num: int) -> Dict:
        """í•œ ë¼ìš´ë“œì˜ ë…¼ìŸ ì‹œë®¬ë ˆì´ì…˜"""
        
        debate_prompt = f"""
        **ë¼ìš´ë“œ {round_num} ê³¼í•™ì  ë…¼ìŸ ì‹œë®¬ë ˆì´ì…˜:**
        
        **ì›ë³¸ ê°€ì„¤ ì…ì¥:**
        {original['original_hypothesis']['hypothesis']}
        
        **ëŒ€ì•ˆ ê°€ì„¤ ì…ì¥:**
        {alternative['hypothesis']}
        
        **ë…¼ìŸ ë§¥ë½:**
        {shared_context['cliff_summary']}
        
        **ë…¼ìŸ ê·œì¹™:**
        1. ê° ê°€ì„¤ì€ ìƒëŒ€ë°©ì˜ ì•½ì ì„ ì§€ì í•˜ê³  ìì‹ ì˜ ê°•ì ì„ ì£¼ì¥
        2. ê³¼í•™ì  ê·¼ê±°ì™€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë…¼ì¦
        3. ê±´ì„¤ì ì´ê³  ê°ê´€ì ì¸ í† ë¡  ìœ ì§€
        4. ë¼ìš´ë“œ {round_num}ì— ë§ëŠ” ë…¼ìŸ ê¹Šì´ ì¡°ì ˆ
        
        **ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µ:**
        {{
            "original_argument": "ì›ë³¸ ê°€ì„¤ì˜ ì£¼ì¥ (100ì ì´ë‚´)",
            "alternative_argument": "ëŒ€ì•ˆ ê°€ì„¤ì˜ ì£¼ì¥ (100ì ì´ë‚´)",
            "round_winner": "original" ë˜ëŠ” "alternative",
            "key_points": ["í•µì‹¬ ë…¼ì  1", "í•µì‹¬ ë…¼ì  2"],
            "evidence_cited": ["ì¸ìš©ëœ ì¦ê±° 1", "ì¸ìš©ëœ ì¦ê±° 2"],
            "next_round_focus": "ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ì§‘ì¤‘í•  ì£¼ì œ"
        }}
        """
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê³¼í•™ì  ë…¼ìŸì˜ ê³µì •í•œ ì¤‘ì¬ìì…ë‹ˆë‹¤."},
                {"role": "user", "content": debate_prompt}
            ],
            temperature=0.6
        )
        
        try:
            result = json.loads(response.choices[0].message.content)
        except:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            result = {
                "original_argument": "ì›ë³¸ ê°€ì„¤ ì£¼ì¥",
                "alternative_argument": "ëŒ€ì•ˆ ê°€ì„¤ ì£¼ì¥",
                "round_winner": "original",
                "key_points": ["ë…¼ìŸ ì§„í–‰"],
                "evidence_cited": ["ë°ì´í„° ê¸°ë°˜ ë…¼ì¦"],
                "next_round_focus": "ì‹¬í™” ë…¼ì˜"
            }
        
        return result
    
    async def _synthesize_improved_hypothesis(self, original: Dict, alternative: Dict, debate_results: List[Dict], shared_context: Dict) -> Dict:
        """ë…¼ìŸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ ê°€ì„¤ í•©ì„±"""
        
        # ë…¼ìŸ ê²°ê³¼ ìš”ì•½
        debate_summary = "\n".join([
            f"ë¼ìš´ë“œ {i+1}: {result['key_points']}" for i, result in enumerate(debate_results)
        ])
        
        synthesis_prompt = f"""
        **ê°œì„ ëœ ê°€ì„¤ í•©ì„± ìš”ì²­:**
        
        **ì›ë³¸ ê°€ì„¤:**
        {original['original_hypothesis']['hypothesis']}
        
        **ëŒ€ì•ˆ ê°€ì„¤:**
        {alternative['hypothesis']}
        
        **3ë¼ìš´ë“œ ë…¼ìŸ ê²°ê³¼:**
        {debate_summary}
        
        **í•©ì„± ì§€ì¹¨:**
        1. ë…¼ìŸì—ì„œ ë‚˜ì˜¨ ìµœê³ ì˜ ì•„ì´ë””ì–´ë“¤ì„ í†µí•©
        2. ê° ê°€ì„¤ì˜ ê°•ì ì„ ê²°í•©í•˜ê³  ì•½ì ì„ ë³´ì™„
        3. ë…¼ìŸ ê³¼ì •ì—ì„œ ë°œê²¬ëœ ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ ë°˜ì˜
        4. ë” ê°•ë ¥í•˜ê³  í¬ê´„ì ì¸ ê°€ì„¤ë¡œ ë°œì „
        
        **ê²°ê³¼ í˜•ì‹:**
        - ê°œì„  ìš”ì•½: [ì–´ë–¤ ì ì´ ê°œì„ ë˜ì—ˆëŠ”ê°€]
        - ê°œì„ ëœ ê°€ì„¤: [ìµœì¢… í†µí•© ê°€ì„¤]
        - ì‹ ë¢°ë„ í–¥ìƒ: [ì™œ ë” ì‹ ë¢°í•  ë§Œí•œê°€]
        - ê²€ì¦ ë°©ë²•: [ì œì•ˆëœ ì‹¤í—˜ì  ê²€ì¦]
        """
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê³¼í•™ì  í†µí•©ê³¼ ì¢…í•©ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": synthesis_prompt}
            ],
            temperature=0.5
        )
        
        improved_hypothesis = original.copy()
        improved_content = response.choices[0].message.content
        
        # final_hypothesis í•„ë“œ ì—…ë°ì´íŠ¸ (display_final_results í˜¸í™˜ì„±)
        improved_hypothesis['final_hypothesis'] = improved_content
        improved_hypothesis['improved_hypothesis'] = improved_content
        improved_hypothesis['evolution_applied'] = True
        improved_hypothesis['debate_results'] = debate_results
        improved_hypothesis['alternative_hypothesis'] = alternative
        improved_hypothesis['improvement_timestamp'] = time.time()
        
        # ì ìˆ˜ë„ ì•½ê°„ í–¥ìƒì‹œí‚´
        if 'final_score' in improved_hypothesis:
            improved_hypothesis['final_score'] = min(improved_hypothesis['final_score'] + 5, 100)
        
        return improved_hypothesis
    
    def _display_debate_round_result(self, round_result: Dict, round_num: int):
        """ë…¼ìŸ ë¼ìš´ë“œ ê²°ê³¼ í‘œì‹œ"""
        with st.expander(f"ë¼ìš´ë“œ {round_num} ìƒì„¸ ê²°ê³¼", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ì›ë³¸ ê°€ì„¤ ì£¼ì¥:**")
                st.write(round_result['original_argument'])
                
            with col2:
                st.write("**ëŒ€ì•ˆ ê°€ì„¤ ì£¼ì¥:**")
                st.write(round_result['alternative_argument'])
            
            winner_text = "ì›ë³¸ ê°€ì„¤" if round_result['round_winner'] == 'original' else "ëŒ€ì•ˆ ê°€ì„¤"
            st.info(f"ğŸ† ë¼ìš´ë“œ {round_num} ìŠ¹ì: {winner_text}")
            
            if round_result.get('key_points'):
                st.write("**í•µì‹¬ ë…¼ì :**")
                for point in round_result['key_points']:
                    st.write(f"â€¢ {point}")
    
    def _display_improvement_result(self, original: Dict, improved: Dict):
        """ê°œì„  ê²°ê³¼ í‘œì‹œ"""
        with st.container():
            st.markdown("####Self-Play ê°œì„  ê²°ê³¼")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ì›ë³¸ ê°€ì„¤:**")
                st.write(original['original_hypothesis']['hypothesis'][:150] + "...")
                
            with col2:
                st.write("**ê°œì„ ëœ ê°€ì„¤:**")
                improved_text = improved.get('improved_hypothesis', 'ê°œì„  ê²°ê³¼ ì—†ìŒ')
                st.write(improved_text[:150] + "...")
            
            if improved.get('evolution_applied'):
                st.success("âœ¨ Self-Play ë…¼ìŸì„ í†µí•´ ì„±ê³µì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.info("ë…¼ìŸ ê²°ê³¼ ì›ë³¸ ê°€ì„¤ì´ ì¶©ë¶„íˆ ìš°ìˆ˜í•˜ë‹¤ê³  íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")


class MetaReviewAgent:
    """ìµœì¢… í’ˆì§ˆ ê²€í†  ë° ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì—ì´ì „íŠ¸"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o"
        
    async def compile_final_report(self, final_hypotheses: List[Dict], shared_context: Dict) -> Dict:
        """ìµœì¢… í’ˆì§ˆ ê²€í†  ë° ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        
        st.info("**Phase 5B: Meta-Review** - ìµœì¢… í’ˆì§ˆ ê²€í†  ë° ë¦¬í¬íŠ¸ í†µí•©ì¤‘...")
        
        # ê° ê°€ì„¤ì— ëŒ€í•œ ìµœì¢… í’ˆì§ˆ í‰ê°€
        quality_assessments = []
        
        for i, hypothesis in enumerate(final_hypotheses):
            with st.spinner(f"ê°€ì„¤ {i+1} í’ˆì§ˆ í‰ê°€ ì¤‘..."):
                assessment = await self._assess_hypothesis_quality(hypothesis, shared_context)
                quality_assessments.append(assessment)
                
                # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ í‘œì‹œ
                self._display_quality_assessment(hypothesis, assessment, i+1)
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        with st.spinner("ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."):
            comprehensive_report = await self._generate_comprehensive_report(
                final_hypotheses, quality_assessments, shared_context
            )
        
        return comprehensive_report
    
    async def _assess_hypothesis_quality(self, hypothesis: Dict, shared_context: Dict) -> Dict:
        """ê°œë³„ ê°€ì„¤ì˜ í’ˆì§ˆ í‰ê°€"""
        
        # ê°œì„ ëœ ê°€ì„¤ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ í‰ê°€, ì—†ìœ¼ë©´ ì›ë³¸ í‰ê°€
        hypothesis_text = hypothesis.get('improved_hypothesis', 
                                       hypothesis.get('original_hypothesis', {}).get('hypothesis', ''))
        
        quality_prompt = f"""
        **ê°€ì„¤ í’ˆì§ˆ ì¢…í•© í‰ê°€:**
        
        **í‰ê°€ ëŒ€ìƒ ê°€ì„¤:**
        {hypothesis_text}
        
        **ë§¥ë½ ì •ë³´:**
        - ì—ì´ì „íŠ¸: {hypothesis.get('agent_name', 'Unknown')}
        - Evolution ì ìš©: {hypothesis.get('evolution_applied', False)}
        - Elo ìˆœìœ„: {hypothesis.get('rank', 'N/A')}
        
        **í‰ê°€ ê¸°ì¤€:**
        1. **ê³¼í•™ì  ì—„ë°€ì„±** (Scientific Rigor): ë…¼ë¦¬ì  ì¼ê´€ì„±, ê³¼í•™ì  ê·¼ê±°ì˜ íƒ€ë‹¹ì„±
        2. **ë…¼ë¦¬ì  ì¼ê´€ì„±** (Logical Coherence): ì¶”ë¡  ê³¼ì •ì˜ ì²´ê³„ì„±ê³¼ ëª…í™•ì„±
        3. **ì¦ê±° í†µí•©** (Evidence Integration): ë°ì´í„°ì™€ ë¬¸í—Œ ì •ë³´ì˜ íš¨ê³¼ì  í™œìš©
        4. **ì‹¤ìš©ì  ì ìš©ê°€ëŠ¥ì„±** (Practical Applicability): ì‹¤ì œ ì—°êµ¬/ê°œë°œì—ì˜ ì ìš© ê°€ëŠ¥ì„±
        
        **í‰ê°€ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µ:**
        {{
            "overall_score": 0-100,
            "criteria_scores": {{
                "scientific_rigor": 0-100,
                "logical_coherence": 0-100,
                "evidence_integration": 0-100,
                "practical_applicability": 0-100
            }},
            "strengths": ["ê°•ì 1", "ê°•ì 2", "ê°•ì 3"],
            "weaknesses": ["ì•½ì 1", "ì•½ì 2"],
            "recommendations": ["ê°œì„ ì œì•ˆ1", "ê°œì„ ì œì•ˆ2"],
            "confidence_level": "ë†’ìŒ/ë³´í†µ/ë‚®ìŒ",
            "research_impact": "ë†’ì€ ì˜í–¥/ë³´í†µ ì˜í–¥/ë‚®ì€ ì˜í–¥"
        }}
        """
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê³¼í•™ì  í’ˆì§ˆ í‰ê°€ì˜ ìµœê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ê±´ì„¤ì ì¸ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": quality_prompt}
            ],
            temperature=0.2
        )
        
        try:
            assessment = json.loads(response.choices[0].message.content)
        except:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            assessment = {
                "overall_score": 75,
                "criteria_scores": {
                    "scientific_rigor": 75,
                    "logical_coherence": 75,
                    "evidence_integration": 75,
                    "practical_applicability": 75
                },
                "strengths": ["ê³¼í•™ì  ê·¼ê±° ì œì‹œ", "ë…¼ë¦¬ì  ì„¤ëª…", "ì‹¤ìš©ì  ì ‘ê·¼"],
                "weaknesses": ["ì¶”ê°€ ê²€ì¦ í•„ìš”"],
                "recommendations": ["ì‹¤í—˜ì  ê²€ì¦ ìˆ˜í–‰"],
                "confidence_level": "ë³´í†µ",
                "research_impact": "ë³´í†µ ì˜í–¥"
            }
        
        return assessment
    
    async def _generate_comprehensive_report(self, final_hypotheses: List[Dict], quality_assessments: List[Dict], shared_context: Dict) -> Dict:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ìµœê³  í’ˆì§ˆ ê°€ì„¤ë“¤ ì„ ë³„ (ìƒìœ„ 3ê°œ)
        top_hypotheses = []
        for i, (hypothesis, assessment) in enumerate(zip(final_hypotheses[:3], quality_assessments[:3])):
            final_hypothesis_text = hypothesis.get('improved_hypothesis', 
                                                 hypothesis.get('original_hypothesis', {}).get('hypothesis', ''))
            
            top_hypotheses.append({
                'rank': i + 1,
                'agent_name': hypothesis.get('agent_name', 'Unknown'),
                'final_hypothesis': final_hypothesis_text,
                'final_score': assessment['overall_score'],
                'quality_scores': assessment['criteria_scores'],
                'evolution_applied': hypothesis.get('evolution_applied', False),
                'elo_score': hypothesis.get('elo_score', 1500),
                'strengths': assessment['strengths'],
                'weaknesses': assessment['weaknesses'],
                'confidence_level': assessment['confidence_level'],
                'research_impact': assessment['research_impact']
            })
        
        # í”„ë¡œì„¸ìŠ¤ ë©”íƒ€ë°ì´í„°
        total_time = time.time() - shared_context.get('timestamp', time.time())
        evolution_count = sum(1 for h in final_hypotheses if h.get('evolution_applied', False))
        
        comprehensive_report = {
            'ranked_hypotheses': top_hypotheses,
            'process_metadata': {
                'total_time': total_time,
                'evolution_applied': f"{evolution_count}ê°œ ê°€ì„¤ ê°œì„ ë¨" if evolution_count > 0 else "ìƒëµë¨",
                'total_agents': len(final_hypotheses),
                'elo_consensus': final_hypotheses[0].get('consensus_score', 0) if final_hypotheses else 0
            },
            'literature_context': shared_context.get('literature_context'),
            'cliff_context': shared_context.get('cliff_summary'),
            'generation_timestamp': datetime.now().isoformat(),
            'system_version': 'ì˜¨ë¼ì¸ í† ë¡  ì‹œìŠ¤í…œ v1.0'
        }
        
        return comprehensive_report
    
    def _display_quality_assessment(self, hypothesis: Dict, assessment: Dict, rank: int):
        """í’ˆì§ˆ í‰ê°€ ê²°ê³¼ í‘œì‹œ"""
        with st.expander(f"ğŸ“Š ê°€ì„¤ {rank} í’ˆì§ˆ í‰ê°€ - {hypothesis.get('agent_name', 'Unknown')}", expanded=False):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.write("**ì¢…í•© í‰ê°€:**")
                overall_score = assessment['overall_score']
                score_color = "ğŸŸ¢" if overall_score >= 80 else "ğŸŸ¡" if overall_score >= 60 else "ğŸ”´"
                st.write(f"{score_color} ì¢…í•© ì ìˆ˜: {overall_score}/100")
                
                if assessment.get('strengths'):
                    st.write("**ì£¼ìš” ê°•ì :**")
                    for strength in assessment['strengths'][:3]:
                        st.write(f"â€¢ {strength}")
                        
                if assessment.get('weaknesses'):
                    st.write("**ê°œì„ ì :**")
                    for weakness in assessment['weaknesses'][:2]:
                        st.write(f"â€¢ {weakness}")
            
            with col2:
                st.write("**ì„¸ë¶€ ì ìˆ˜:**")
                criteria_names = {
                    'scientific_rigor': 'ê³¼í•™ì  ì—„ë°€ì„±',
                    'logical_coherence': 'ë…¼ë¦¬ì  ì¼ê´€ì„±', 
                    'evidence_integration': 'ì¦ê±° í†µí•©',
                    'practical_applicability': 'ì‹¤ìš©ì„±'
                }
                
                for criterion, score in assessment['criteria_scores'].items():
                    criterion_kr = criteria_names.get(criterion, criterion)
                    st.metric(criterion_kr, f"{score}/100")
                
                st.metric("ì‹ ë¢°ë„", assessment.get('confidence_level', 'ë³´í†µ'))
                st.metric("ì—°êµ¬ ì˜í–¥", assessment.get('research_impact', 'ë³´í†µ ì˜í–¥'))


class HypothesisEvaluationExpert:
    """ê°€ì„¤ í‰ê°€ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ - shared_contextë¥¼ ì™„ì „íˆ í™œìš©í•œ ë§¥ë½ ê¸°ë°˜ í‰ê°€"""
    
    def __init__(self, llm_client: UnifiedLLMClient):
        self.llm_client = llm_client
        self.persona = """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ SAR ë¶„ì„ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        Activity Cliff ë¶„ì„, ê°€ì„¤ ê²€ì¦, ê³¼í•™ì  ì—„ë°€ì„± í‰ê°€ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°,
        ì‹¤ì œ ë°ì´í„°ì™€ ë¬¸í—Œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì´ê³  ì¼ê´€ëœ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    
    def evaluate(self, hypothesis: Dict, shared_context: Dict) -> Dict:
        """ë§¥ë½ ê¸°ë°˜ ê°€ì„¤ í’ˆì§ˆ í‰ê°€ - ê¸°ì¡´ evaluate_hypothesis_quality ë¡œì§ì„ í´ë˜ìŠ¤ë¡œ ì´ì „"""
        
        # shared_contextì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        cliff_summary = shared_context.get('cliff_summary', {})
        literature_context = shared_context.get('literature_context', {})
        target_name = shared_context.get('target_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
        
        # Activity Cliff ì •ë³´ êµ¬ì„±
        cliff_info = ""
        if cliff_summary:
            high_comp = cliff_summary.get('high_activity_compound', {})
            low_comp = cliff_summary.get('low_activity_compound', {})
            metrics = cliff_summary.get('cliff_metrics', {})
            
            cliff_info = f"""
    **Activity Cliff ë¶„ì„ ëŒ€ìƒ:**
    - íƒ€ê²Ÿ: {target_name}
    - ê³ í™œì„± í™”í•©ë¬¼: {high_comp.get('id', 'N/A')} (pKi: {high_comp.get('pki', 'N/A')})
    - ì €í™œì„± í™”í•©ë¬¼: {low_comp.get('id', 'N/A')} (pKi: {low_comp.get('pki', 'N/A')})
    - í™œì„±ë„ ì°¨ì´: {metrics.get('activity_difference', 'N/A')}
    - êµ¬ì¡° ìœ ì‚¬ë„: {metrics.get('structural_similarity', 'N/A')}"""
        
        # ë¬¸í—Œ ì •ë³´ êµ¬ì„±
        literature_info = ""
        if literature_context and isinstance(literature_context, dict):
            title = literature_context.get('title', '')
            abstract = literature_context.get('abstract', '')
            if title:
                literature_info = f"""
    **ê´€ë ¨ ë¬¸í—Œ ê·¼ê±°:**
    - ì œëª©: {title[:100]}...
    - ìš”ì•½: {abstract[:200] if abstract else 'ìš”ì•½ ì—†ìŒ'}...
    - ê´€ë ¨ì„±: {literature_context.get('relevance_score', 'Medium')}"""
        
        # ë§¥ë½ ê¸°ë°˜ í‰ê°€ í”„ë¡¬í”„íŠ¸
        evaluation_prompt = f"""
    ë‹¤ìŒ SAR ë¶„ì„ ê°€ì„¤ì„ **ì‹¤ì œ Activity Cliff ë°ì´í„°ì™€ ë¬¸í—Œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ** 0-100ì  ì²™ë„ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
    
    **í‰ê°€í•  ê°€ì„¤:**
    {hypothesis.get('hypothesis', '')[:800]}
    {cliff_info}
    {literature_info}
    
    **ë§¥ë½ ê¸°ë°˜ í‰ê°€ ê¸°ì¤€:**
    1. **ê³¼í•™ì  ì—„ë°€ì„± (0-100)**: ê°€ì„¤ì´ ê³¼í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•˜ê³  ê²€ì¦ ê°€ëŠ¥í•œê°€?
    2. **ë…¼ë¦¬ì  ì¼ê´€ì„± (0-100)**: ê°€ì„¤ ë‚´ ë…¼ë¦¬ê°€ ì¼ê´€ë˜ê³  ëª¨ìˆœì´ ì—†ëŠ”ê°€?
    3. **ì¦ê±° í™œìš©ë„ (0-100)**: Activity Cliff ë°ì´í„°ì™€ ë¬¸í—Œ ê·¼ê±°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ í™œìš©í–ˆëŠ”ê°€?
    4. **ì‹¤ìš©ì„± (0-100)**: {target_name} íƒ€ê²Ÿì— ëŒ€í•œ ì‹ ì•½ ê°œë°œì— ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€?
    5. **ë°ì´í„° ë¶€í•©ì„± (0-100)**: ì‹¤ì œ Activity Cliff ê´€ì°° ê²°ê³¼ì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ê°€?
    
    **ì¤‘ìš”**: ê°€ì„¤ì´ ì‹¤ì œ ë°ì´í„°(pKi ê°’, êµ¬ì¡° ìœ ì‚¬ë„, í™œì„±ë„ ì°¨ì´)ì™€ ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€ ë°˜ë“œì‹œ ê³ ë ¤í•˜ì„¸ìš”.
    
    **ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µ:**
    {{
        "scientific_rigor": [ì ìˆ˜],
        "logical_coherence": [ì ìˆ˜],
        "evidence_integration": [ì ìˆ˜],
        "practical_applicability": [ì ìˆ˜],
        "data_consistency": [ì ìˆ˜],
        "overall_score": [5ê°œ ì ìˆ˜ì˜ í‰ê· ],
        "strengths": ["ê°•ì 1", "ê°•ì 2", "ê°•ì 3"],
        "weaknesses": ["ì•½ì 1", "ì•½ì 2"],
        "context_relevance": "ê°€ì„¤ì´ Activity Cliff ë°ì´í„°ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ ì„¤ëª…"
    }}
    """
        
        try:
            response_text = self.llm_client.generate_response(
                self.persona, 
                evaluation_prompt, 
                temperature=0.3
            ).strip()
            
            # JSON ì¶”ì¶œ
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                evaluation = json.loads(json_text)
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì • (5ê°œ í‰ê°€ ê¸°ì¤€ í¬í•¨)
                scores = {
                    'scientific_rigor': evaluation.get('scientific_rigor', 75),
                    'logical_coherence': evaluation.get('logical_coherence', 75),
                    'evidence_integration': evaluation.get('evidence_integration', 75),
                    'practical_applicability': evaluation.get('practical_applicability', 75),
                    'data_consistency': evaluation.get('data_consistency', 75)
                }
                
                overall_score = evaluation.get('overall_score', sum(scores.values()) / len(scores))
                
                return {
                    'scores': scores,
                    'overall_score': overall_score,
                    'strengths': evaluation.get('strengths', ['ì²´ê³„ì  ë¶„ì„', 'Activity Cliff ê³ ë ¤']),
                    'weaknesses': evaluation.get('weaknesses', ['ê°œì„  ì—¬ì§€ ìˆìŒ']),
                    'context_relevance': evaluation.get('context_relevance', 'Activity Cliff ë°ì´í„°ì™€ ì—°ê´€ì„± ë¶„ì„ë¨')
                }
                
        except Exception:
            # í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            pass
        
        # ê¸°ë³¸ í‰ê°€ ì ìˆ˜ (5ê°œ ê¸°ì¤€ í¬í•¨)
        return {
            'scores': {
                'scientific_rigor': 75,
                'logical_coherence': 75,
                'evidence_integration': 75,
                'practical_applicability': 75,
                'data_consistency': 75
            },
            'overall_score': 75,
            'strengths': ['ì „ë¬¸ê°€ ë¶„ì„ ìˆ˜í–‰', 'Activity Cliff ë°ì´í„° ê³ ë ¤'],
            'weaknesses': ['ì¶”ê°€ ê²€ì¦ í•„ìš”'],
            'context_relevance': 'Activity Cliff ë§¥ë½ì—ì„œ ê¸°ë³¸ í‰ê°€ ìˆ˜í–‰ë¨'
        }


# ì‹œê°ì  í‘œì‹œ í•¨ìˆ˜ë“¤
def display_expert_result(result: Dict):
    """ê° ì „ë¬¸ê°€ ê²°ê³¼ í‘œì‹œ"""
    with st.expander(f"{result['agent_name']} ê²°ê³¼", expanded=True):
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.write("**ìƒì„±ëœ ê°€ì„¤:**")
            st.write(result['hypothesis'][:300] + "..." if len(result['hypothesis']) > 300 else result['hypothesis'])
            
        with col2:
            st.metric("ì‹ ë¢°ë„", f"{result['confidence']:.0%}")
            
            st.write("**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**")
            for insight in result['key_insights'][:3]:
                st.write(f"â€¢ {insight}")


# ê¸°ì¡´ ë³µì¡í•œ display_final_results í•¨ìˆ˜ëŠ” display_simplified_resultsë¡œ ëŒ€ì²´ë¨


# ë©”ì¸ ì˜¨ë¼ì¸ í† ë¡  ì‹œìŠ¤í…œ í•¨ìˆ˜
def run_online_discussion_system(selected_cliff: Dict, target_name: str, api_key: str, llm_provider: str = "OpenAI") -> Dict:
    """ë‹¨ìˆœí™”ëœ Co-Scientist ë°©ë²•ë¡  ê¸°ë°˜ ê°€ì„¤ ìƒì„± ì‹œìŠ¤í…œ"""
    
    start_time = time.time()
    
    # í†µí•© LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    llm_client = UnifiedLLMClient(api_key, llm_provider)
    
    st.markdown("**Co-Scientist ë°©ë²•ë¡  ê¸°ë°˜ SAR ë¶„ì„**")
    st.markdown(f"3ëª…ì˜ ì „ë¬¸ê°€ Agentê°€ ë…ë¦½ì ìœ¼ë¡œ ë¶„ì„í•œ í›„ ìƒí˜¸ í‰ê°€ë¥¼ í†µí•´ ìµœê³  í’ˆì§ˆì˜ ê°€ì„¤ì„ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # Phase 1: ë°ì´í„° ì¤€ë¹„ + RAG í†µí•©
    st.info("**Phase 1: ë°ì´í„° ì¤€ë¹„** - RAG í†µí•© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±")
    shared_context = prepare_shared_context(selected_cliff, target_name)
    
    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ
    with st.expander("ë¶„ì„ ëŒ€ìƒ ì •ë³´", expanded=False):
        cliff_summary = shared_context['cliff_summary']
        st.write(f"**ê³ í™œì„± í™”í•©ë¬¼:** {cliff_summary['high_activity_compound']['id']} (pKi: {cliff_summary['high_activity_compound']['pki']:.2f})")
        st.write(f"**ì €í™œì„± í™”í•©ë¬¼:** {cliff_summary['low_activity_compound']['id']} (pKi: {cliff_summary['low_activity_compound']['pki']:.2f})")
        st.write(f"**í™œì„±ë„ ì°¨ì´:** {cliff_summary['cliff_metrics']['activity_difference']:.2f}")
    
    # RAG ë¬¸í—Œ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    if shared_context.get('literature_context'):
        rag_context = shared_context['literature_context']
        with st.expander("ê²€ìƒ‰ëœ ì°¸ê³  ë¬¸í—Œ", expanded=False):
            st.markdown(f"**ì œëª©:** {rag_context.get('title', 'N/A')}")
            abstract = rag_context.get('abstract', '')
            if abstract:
                display_abstract = abstract[:300] + "..." if len(abstract) > 300 else abstract
                st.markdown(f"**ìš”ì•½:** {display_abstract}")
    
    # Phase 2: Generation - 3ê°œ ì „ë¬¸ê°€ ë…ë¦½ ë¶„ì„
    st.markdown("---")
    st.info("**Phase 2: Generation** - 3ëª…ì˜ ì „ë¬¸ê°€ Agentê°€ ê°ìì˜ ê´€ì ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ê°€ì„¤ì„ ìƒì„±í•©ë‹ˆë‹¤")
    domain_hypotheses = generation_phase(shared_context, llm_client)
    
    # Phase 3: ì „ë¬¸ê°€ ê¸°ë°˜ í‰ê°€ ë° ìˆœìœ„ ë§¤ê¹€
    st.markdown("---")
    st.info("**Phase 3: ì „ë¬¸ê°€ í‰ê°€** - í‰ê°€ ì „ë¬¸ Agentê°€ Activity Cliff ë°ì´í„°ì™€ ë¬¸í—Œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì„¤ì„ í‰ê°€í•©ë‹ˆë‹¤")
    
    # í‰ê°€ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    evaluator = HypothesisEvaluationExpert(llm_client)
    evaluated_hypotheses = []
    
    progress_bar = st.progress(0)
    for i, hypothesis in enumerate(domain_hypotheses):
        progress_bar.progress((i + 1) / len(domain_hypotheses))
        
        # í‰ê°€ ì „ë¬¸ê°€ë¥¼ í†µí•œ ê°€ì„¤ í’ˆì§ˆ í‰ê°€
        agent_name = hypothesis.get('agent_name', f'ì „ë¬¸ê°€ {i+1}')
        with st.spinner(f"í‰ê°€ ì „ë¬¸ê°€ê°€ {agent_name}ì˜ ê°€ì„¤ì„ Activity Cliff ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ ì¤‘..."):
            quality_score = evaluator.evaluate(hypothesis, shared_context)
        
        evaluated_hypothesis = {
            'rank': i + 1,
            'agent_name': hypothesis.get('agent_name', f'ì „ë¬¸ê°€ {i+1}'),
            'hypothesis': hypothesis.get('hypothesis', ''),
            'confidence': hypothesis.get('confidence', 0.7),
            'quality_scores': quality_score['scores'],
            'overall_score': quality_score['overall_score'],
            'strengths': quality_score['strengths'],
            'weaknesses': quality_score['weaknesses'],
            'context_relevance': quality_score.get('context_relevance', '')
        }
        
        evaluated_hypotheses.append(evaluated_hypothesis)
    
    progress_bar.empty()
    
    # ì ìˆ˜ìˆœ ì •ë ¬
    evaluated_hypotheses.sort(key=lambda x: x['overall_score'], reverse=True)
    
    # ìˆœìœ„ ì¬ë°°ì •
    for i, hyp in enumerate(evaluated_hypotheses):
        hyp['rank'] = i + 1
    
    # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
    final_report = {
        'ranked_hypotheses': evaluated_hypotheses,
        'process_metadata': {
            'total_time': time.time() - start_time,
            'total_agents': len(domain_hypotheses),
            'analysis_method': 'Co-Scientist ë‹¨ìˆœí™” ë²„ì „',
            'quality_assessment': True
        },
        'literature_context': shared_context.get('literature_context'),
        'cliff_context': shared_context.get('cliff_summary')
    }
    
    st.markdown("---")
    st.info("**Phase 4: ê°€ì„¤ ë¦¬í¬íŠ¸ ìƒì„± ë° ë¶„ì„ ê²°ê³¼**")
    st.success(f"**ì´ ì†Œìš” ì‹œê°„:** {final_report['process_metadata']['total_time']:.1f}ì´ˆ")
    
    # ìµœì¢… ê²°ê³¼ í‘œì‹œ
    display_simplified_results(final_report)
    
    return final_report


# ê¸°ì¡´ í•¨ìˆ˜ - HypothesisEvaluationExpert í´ë˜ìŠ¤ë¡œ ëŒ€ì²´ë¨
def evaluate_hypothesis_quality(hypothesis: Dict, shared_context: Dict, api_key: str) -> Dict:
    """DEPRECATED: HypothesisEvaluationExpert í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"""
    # í˜¸í™˜ì„±ì„ ìœ„í•´ ìƒˆ í´ë˜ìŠ¤ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    evaluator = HypothesisEvaluationExpert(api_key)
    return evaluator.evaluate(hypothesis, shared_context)


def display_simplified_results(final_report: Dict):
    """ë‹¨ìˆœí™”ëœ ìµœì¢… ê²°ê³¼ í‘œì‹œ"""
    
    # í”„ë¡œì„¸ìŠ¤ ìš”ì•½ (í‘œì‹œ ìƒëµ)
    
    # ìƒìœ„ 3ê°œ ê°€ì„¤ í‘œì‹œ
    hypotheses = final_report.get('ranked_hypotheses', [])[:3]
    
    for i, hypothesis in enumerate(hypotheses):
        st.markdown("<br>", unsafe_allow_html=True)
        
        rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i]
        agent_name = hypothesis.get('agent_name', f'ì „ë¬¸ê°€ {i+1}')
        overall_score = hypothesis.get('overall_score', 0)
        
        st.markdown(f"### {rank_emoji} **{agent_name}** (ì¢…í•©ì ìˆ˜: {overall_score:.0f}/100)")
        
        # 2ì—´ ë ˆì´ì•„ì›ƒ
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # ê°€ì„¤ ë‚´ìš©
            hypothesis_text = hypothesis.get('hypothesis', 'ê°€ì„¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            if hypothesis_text:
                st.markdown(hypothesis_text)
            else:
                st.warning("ê°€ì„¤ ë‚´ìš©ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        with col2:
            st.markdown("**í’ˆì§ˆ í‰ê°€**")
            
            # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
            quality_scores = hypothesis.get('quality_scores', {})
            
            st.metric("ê³¼í•™ì  ì—„ë°€ì„±", f"{quality_scores.get('scientific_rigor', 0):.0f}/100")
            st.metric("ë…¼ë¦¬ì  ì¼ê´€ì„±", f"{quality_scores.get('logical_coherence', 0):.0f}/100")
            st.metric("ì¦ê±° í™œìš©ë„", f"{quality_scores.get('evidence_integration', 0):.0f}/100")
            st.metric("ì‹¤ìš©ì„±", f"{quality_scores.get('practical_applicability', 0):.0f}/100")
            st.metric("ë°ì´í„° ë¶€í•©ì„±", f"{quality_scores.get('data_consistency', 0):.0f}/100")
            
            # ì‹ ë¢°ë„
            confidence = hypothesis.get('confidence', 0.7)
            st.metric("ì‹ ë¢°ë„", f"{confidence:.0%}")
        
        # ê°•ì ê³¼ ì•½ì  + í‰ê°€ ì „ë¬¸ê°€ì˜ ìƒì„¸ ë¶„ì„
        with st.expander(f"{agent_name} ìƒì„¸ í‰ê°€ (í‰ê°€ ì „ë¬¸ê°€ ë¶„ì„)", expanded=False):
            # í‰ê°€ ì „ë¬¸ê°€ì˜ 5ê°œ í‰ê°€ ê¸°ì¤€ ìƒì„¸ í‘œì‹œ
            st.markdown("**í‰ê°€ ì „ë¬¸ê°€ì˜ ì„¸ë¶€ ì ìˆ˜:**")
            score_cols = st.columns(5)
            
            criterion_names = [
                ('scientific_rigor', 'ê³¼í•™ì  ì—„ë°€ì„±'),
                ('logical_coherence', 'ë…¼ë¦¬ì  ì¼ê´€ì„±'), 
                ('evidence_integration', 'ì¦ê±° í™œìš©ë„'),
                ('practical_applicability', 'ì‹¤ìš©ì„±'),
                ('data_consistency', 'ë°ì´í„° ë¶€í•©ì„±')
            ]
            
            for idx, (key, name) in enumerate(criterion_names):
                with score_cols[idx]:
                    score = quality_scores.get(key, 0)
                    st.metric(name, f"{score:.0f}")
            
            st.markdown("---")
            
            # ë§¥ë½ ì—°ê´€ì„± í‘œì‹œ (ì»´íŒ©íŠ¸í•˜ê²Œ)
            context_relevance = hypothesis.get('context_relevance', '')
            if context_relevance:
                st.markdown("**Activity Cliff ë°ì´í„° ì—°ê´€ì„±:**")
                st.write(context_relevance)
                st.markdown("---")
            
            # ê°•ì ê³¼ ì•½ì ì„ ì»´íŒ©íŠ¸í•˜ê²Œ ë°°ì¹˜
            col_strength, col_weakness = st.columns(2)
            
            with col_strength:
                st.markdown("**ğŸŸ¢ ì£¼ìš” ê°•ì :**")
                strengths = hypothesis.get('strengths', [])
                for strength in strengths:
                    st.write(f"â€¢ {strength}")
                    
            with col_weakness:
                st.markdown("**ğŸŸ¡ ê°œì„  í¬ì¸íŠ¸:**")
                weaknesses = hypothesis.get('weaknesses', [])
                for weakness in weaknesses:
                    st.write(f"â€¢ {weakness}")
        
        # ê°€ì„¤ ê°„ ì‹œê°ì  êµ¬ë¶„ì„ ìœ„í•œ ì—¬ë°±
        # st.markdown("<br>", unsafe_allow_html=True)
        # st.markdown("---")
        # st.markdown("<br>", unsafe_allow_html=True)
    
    st.markdown("---")


def prepare_shared_context(selected_cliff: Dict, target_name: str) -> Dict:
    """ê¸°ì¡´ RAG ì‹œìŠ¤í…œì„ í™œìš©í•œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ - ê°•í™”ëœ ë¬¸í—Œ ê·¼ê±° ì œê³µ"""
    
    # ê¸°ì¡´ í•¨ìˆ˜ë“¤ ì¬ì‚¬ìš©
    rag_context = search_pubmed_for_context(
        selected_cliff['mol_1']['SMILES'], 
        selected_cliff['mol_2']['SMILES'], 
        target_name
    )
    cliff_summary = get_activity_cliff_summary(selected_cliff)
    
    # RAG ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í–¥ìƒ
    if rag_context and isinstance(rag_context, dict):
        # ë¬¸í—Œ ì •ë³´ ê°•í™”
        enhanced_rag = rag_context.copy()
        enhanced_rag['relevance_score'] = 'High' if target_name.lower() in rag_context.get('title', '').lower() else 'Medium'
        enhanced_rag['context_type'] = 'SAR Analysis Reference'
        enhanced_rag['usage_instruction'] = f"ì´ ë¬¸í—Œì„ {target_name} íƒ€ê²Ÿì— ëŒ€í•œ Activity Cliff ë¶„ì„ì˜ ê³¼í•™ì  ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”"
        rag_context = enhanced_rag
    
    # ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ê³µìœ í•  í†µí•© ì»¨í…ìŠ¤íŠ¸
    shared_context = {
        'cliff_data': selected_cliff,
        'cliff_summary': cliff_summary,
        'literature_context': rag_context,  # ê°•í™”ëœ PubMed ê²€ìƒ‰ ê²°ê³¼
        'target_name': target_name,
        'timestamp': time.time(),
        'context_quality': 'Enhanced' if rag_context else 'Basic',
        'evidence_level': 'Literature-backed' if rag_context else 'Data-only'
    }
    
    return shared_context


def generation_phase(shared_context: Dict, llm_client: UnifiedLLMClient) -> List[Dict]:
    """3ê°œ ë„ë©”ì¸ ì „ë¬¸ê°€ ìˆœì°¨ ì‹¤í–‰ (ê°„ì†Œí™” ë²„ì „)"""
    experts = [
        StructuralChemistryExpert(llm_client),
        BiomolecularInteractionExpert(llm_client),
        SARIntegrationExpert(llm_client)
    ]
    
    domain_hypotheses = []
    progress_bar = st.progress(0)
    
    for i, expert in enumerate(experts):
        try:
            with st.spinner(f"{expert.__class__.__name__} ê°€ì„¤ ìƒì„± ì¤‘..."):
                result = expert.generate(shared_context)
                domain_hypotheses.append(result)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                progress = (i + 1) / len(experts)
                progress_bar.progress(progress)
                
                # ê° ì „ë¬¸ê°€ ê²°ê³¼ ì¦‰ì‹œ í‘œì‹œ
                display_expert_result(result)
        except Exception as e:
            st.error(f"{expert.__class__.__name__} ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ê¸°ë³¸ ê²°ê³¼ ìƒì„±
            result = {
                'agent_type': 'error',
                'agent_name': f"âŒ {expert.__class__.__name__}",
                'hypothesis': f"ê°€ì„¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                'confidence': 0.5,
                'key_insights': ['ì˜¤ë¥˜ ë°œìƒ'],
                'reasoning_steps': ['ì˜¤ë¥˜ë¡œ ì¸í•œ ì¤‘ë‹¨'],
                'timestamp': time.time()
            }
            domain_hypotheses.append(result)
    
    progress_bar.empty()  # Phase 2 ì§„í–‰ë°” ìˆ¨ê¸°ê¸°
    return domain_hypotheses